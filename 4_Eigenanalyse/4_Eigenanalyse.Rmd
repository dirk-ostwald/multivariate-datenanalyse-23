---
fontsize: 8pt
bibliography: 4_Referenzen.bib
citation_package: natbib
output:
  beamer_presentation:
    keep_tex: true
    includes:
      in_header: 4_Header.tex
---


```{r, include = F}
source("4_R_common.R")
```

#  {.plain}
\center
```{r, echo = FALSE, out.width = "20%"}
knitr::include_graphics("4_Abbildungen/mvda_4_otto.png")
```

\vspace{2mm}

\Huge
Multivariate Datenanalyse
\vspace{6mm}

\large
MSc Psychologie WiSe 2021/22

\vspace{6mm}
\large
Prof. Dr. Dirk Ostwald

#  {.plain}

\vfill
\center
\huge
\textcolor{black}{(4) Eigenanalyse}
\vfill

# 
\vfill
\setstretch{2.5}
\Large
Eigenvektoren und Eigenwerte

Orthonormalzerlegung 

Singulärwertzerlegung

Vektorkoordinatentransformation

Selbstkontrollfragen
\vfill 

# 
\vfill
\setstretch{2.5}
\Large
**Eigenvektoren und Eigenwerte**

Orthonormalzerlegung 

Singulärwertzerlegung

Vektorkoordinatentransformation

Selbstkontrollfragen
\vfill 

# Eigenvektoren und Eigenwerte
\small
\begin{definition}[Eigenvektor, Eigenwert]
$A \in \mathbb{R}^{m \times m}$ sei eine quadratische Matrix. Dann heißt jeder
Vektor $v \in \mathbb{R}^m, v \neq 0$, für den gilt, dass
\begin{equation}
Av  = \lambda v
\end{equation}
mit $\lambda \in \mathbb{R}$ ein \textit{Eigenvektor} von $A$. $\lambda$
heißt zugehöriger \textit{Eigenwert} von $A$.
\end{definition}

\small
Bemerkungen

* Ein Eigenvektor $v$ von $A$ wird durch $A$ mit einem Faktor $\lambda$ verlängert.
* Jeder Eigenvektor hat einen zugehörigen Eigenwert.
* Die Eigenwerte  verschiedener Eigenvektor können identisch sein.

# Eigenvektoren und Eigenwerte
\small
\begin{theorem}[Multiplikativität von Eigenvektoren]
\justifying
\normalfont
$A \in \mathbb{R}^{m \times m}$ sei eine quadratische Matrix. Wenn $v \in \mathbb{R}^m$
Eigenvektor von $A$ mit Eigenwert $\lambda \in \mathbb{R}$ ist, dann ist auch $av \in \mathbb{R}^m $
mit $a \in \mathbb{R}$ Eigenvektor von $A$ und zwar mit Eigenwert $a\lambda \in \mathbb{R}$.
\end{theorem}

\footnotesize
\underline{Beweis}

Es gilt
\begin{equation}
Av = \lambda v \Leftrightarrow a(Av) = a(\lambda)v \Leftrightarrow A(av) = (a\lambda)v
\end{equation}
Also ist $av$ ein Eigenvektor von $A$ mit Eigenwert $a\lambda$.

$\hfill\Box$

\small

**Konvention**

Wir betrachten im Folgenden nur Eigenvektoren mit $\Vert v \Vert = 1$.


# Eigenvektoren und Eigenwerte {.t}
\vspace{1mm}

\textcolor{darkblue}{Visualisierung eines Eigenvektors}
\footnotesize
\center

Für $A := \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$ ist 
$v := \frac{1}{\sqrt{2}} \begin{pmatrix} 1 \\ 1 \end{pmatrix}$ 
Eigenvektor zum Eigenwert $\lambda = 3, w := \begin{pmatrix} 1 \\ 0 \end{pmatrix}$ ist kein Eigenvektor.


```{r, eval = F, echo = F}
library(latex2exp)
graphics.off()
fdir        =  file.path(getwd(), "4_Abbildungen")
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = .95,
cex.main    = 1.2)

# Definitionen
A           = matrix(c(2,1,
                       1,2),
                     byrow = TRUE,
                     nrow = 2) 
v           = matrix(c(1/sqrt(2),
                       1/sqrt(2)),
                       nrow = 2)
w           = matrix(c(1,
                       0),
                       nrow = 2)
Av          = A %*% v
Aw          = A %*% w

# Punktperspektive
plot(
NULL,
xlab        = TeX("$x_1$"),
ylab        = TeX("$x_2$"),
xlim        = c(0,3),
ylim        = c(0,3))
grid()
points(
c(v[1], w[1], Av[1], Aw[1]),
c(v[2], w[2], Av[2], Aw[2]),
pch = 19,
xpd = TRUE)
text(1/sqrt(2), 0.9, TeX("$v$"))
text(1        , 0.2, TeX("$w$"))
text(3/sqrt(2), 2.4, TeX("$Av = \\lambda v$"))
text(2        , 1.3, TeX("$Aw \\neq \\lambda w$"))

# Pfeilperspektive
arrows(
x0          = c(0,0,0,0),
y0          = c(0,0,0,0),
x1          = c(v[1],w[1],Av[1],Aw[1]),
y1          = c(v[2],w[2],Av[2],Aw[2]),
angle       = 20,
length      = .1,
lwd         = 1.5,
col         = c("black", "gray60", "black", "gray60"),
xpd         = TRUE)


# export to pdf
dev.copy2pdf(                                                                    # export to PDF
             file   = file.path(fdir, "mvda_4_eigenvektor.pdf"),                 # filename
             width  = 4,                                                         # PDF width
             height = 4                                                          # PDF height
             )

```

    
```{r, echo = FALSE, out.width = "55%"}  
knitr::include_graphics("4_Abbildungen/mvda_4_eigenvektor.pdf") 
```


# Eigenvektoren und Eigenwerte
\small
\begin{theorem}[Bestimmung von Eigenwerten und Eigenvektoren]
\normalfont
\justifying
$A \in \mathbb{R}^{m \times m}$ sei eine quadratische Matrix. Dann ergeben sich
die Eigenwerte von $A$ als die Nullstellen des \textit{charakteristischen Polynoms}
\begin{equation}
\chi_A(\lambda) := \det(A - \lambda I_m).
\end{equation}
von $A$. Weiterhin seien $\lambda_i^*, i = 1,2,...$ die auf diese Weise bestimmten
Eigenwerte von $A$. Die entsprechenden Eigenvektoren $v_i, i = 1,2,...$ von $A$
können dann durch Lösen der linearen Gleichungssysteme
\begin{equation}
(A - \lambda_i^* I_m)v_i = 0_m \mbox{ für } i = 1,2,...
\end{equation}
bestimmt werden.
\end{theorem}

\footnotesize
Bemerkungen

* Für kleine Matrizen mit $m \le 3$ können Eigenwerte und Eigenvektoren manuell bestimmt werden.
* Bei großen Matrizen werden Eigenwerte und Eigenvektor im Allgemeinen numerisch bestimmt.
* R's `eigen()`, Scipy's `linalg.eig()`, Matlab's `eig()`.

# Eigenvektoren und Eigenwerte
\setstretch{1.2}
\footnotesize
\underline{Beweis}

\noindent (1) Bestimmen von Eigenwerten

Wir halten zunächst fest, dass mit der Definition
von Eigenvektoren und
Eigenwerten gilt, dass
\begin{equation}
Av = \lambda v
\Leftrightarrow Av - \lambda v = 0_m
\Leftrightarrow (A - \lambda I_m)v = 0_m.
\end{equation}
Für den Eigenwert $\lambda$ wird der Eigenvektor $v$ also durch $(A - \lambda I_m)$
auf den Nullvektor $0_m$ abgebildet. Weil aber per Definition $v \neq 0_m$ gilt,
ist die Matrix $(A - \lambda I_m)$ somit nicht invertierbar: sowohl der Nullvektor
als auch $v$ werden durch $A$ auf $0_m$ abgebildet, die Abbildung
\begin{equation}
f : \mathbb{R}^m \to \mathbb{R}^m, x \mapsto (A - \lambda I_m)x
\end{equation}
ist also nicht bijektiv, und $(A - \lambda I_m)^{-1}$ kann nicht existieren.
Die Tatsache, dass $(A - \lambda I_m)$ nicht invertierbar ist, ist aber
äquivalent dazu, dass die Determinante von $(A -\lambda I_m)$ Null ist. Also ist
\begin{equation}
\chi_A(\lambda) = \det(A - \lambda I_m) = 0
\end{equation}
notwendige und hinreichende Bedingung dafür, dass $\lambda$ ein Eigenwert von
$A$ ist.

\noindent (2) Bestimmen von Eigenvektoren

Es sei $\lambda_i^*$ ein Eigenwert von $A$. Dann
gilt mit den obigen Überlegungen, dass Auflösen von
\begin{equation}
(A - \lambda_i^* I_m)v_i^* = 0_m
\end{equation}
nach $v_i^*$ einen Eigenvektor zum Eigenwert $\lambda^*$  ergibt.
$\hfill \Box$

# Eigenvektoren und Eigenwerte
\setstretch{1.6}
Beispiel  

\footnotesize
Es sei
\begin{equation}
A :=
\begin{pmatrix*}[r]
2 & 1 \\
1 & 2\end{pmatrix*}
\end{equation}
Wir wollen die Eigenwerte und Eigenvektoren von $A$ bestimmen.
\vspace{1mm}


\small
\noindent (1) Berechnen von Eigenwerten
\footnotesize

Die Eigenwerte von $A$ sind die Nullstellen des charakteristischen Polynoms von $A$.

Das charakteristische Polynom von $A$ ergibt als
\begin{equation}
\chi_A(\lambda)
=
\det\left(
\begin{pmatrix*}[r]
2 & 1 \\
1 & 2\end{pmatrix*}
-
\begin{pmatrix*}[r]
\lambda & 0 \\
0 		& \lambda
\end{pmatrix*}
\right)
=
\det
\begin{pmatrix*}[r]
2 - \lambda & 1 \\
1 & 2 - \lambda
\end{pmatrix*}
= (2 - \lambda)^2 - 1.
\end{equation}
Nullsetzen und Auflösen nach $\lambda$ ergibt mit der [pq-Formel](https://de.wikipedia.org/wiki/Quadratische_Gleichung)
\begin{equation}
(2 - \lambda)^2 - 1 = 0 \Rightarrow \lambda_1 = 3, \lambda_2 = 1.
\end{equation}
Die Eigenwerte von $A$ sind also $\lambda_1 = 3$ und $\lambda_2 = 1$.


# Eigenvektoren und Eigenwerte
\setstretch{1.6}
Beispiel (fortgeführt)

\small
\noindent (2) Berechnen von Eigenvektoren

\footnotesize
Die Eigenvektoren zu den Eigenwerten $\lambda_1 = 3$ und $\lambda_2 = 1$ ergeben sich
durch Lösen der linearen Gleichungssysteme
\begin{equation}
(A - \lambda_i I_2)v_i = 0_2
\end{equation}
Für $\lambda_1 = 3$ ergibt sich
\begin{equation}
(A - 3I_2)v_1 = 0_2
\Leftrightarrow
\begin{pmatrix*}[r]
-1 & 1 \\
 1 & -1
\end{pmatrix*}
\begin{pmatrix*}[r]
v_{11} \\
v_{12}
\end{pmatrix*}
=
\begin{pmatrix*}[r]
0 \\
0
\end{pmatrix*}
\Rightarrow
v_1 =
\frac{1}{\sqrt{2}}
\begin{pmatrix*}[r]
1 \\
1
\end{pmatrix*}
\mbox{ ist eine Lösung. }
\end{equation}
Fpr $\lambda_2 = 1$ ergibt sich
\begin{equation}
(A - 1I_2)v_2 = 0_2
\Leftrightarrow
\begin{pmatrix*}[r]
1 & 1 \\
1 & 1
\end{pmatrix*}
\begin{pmatrix*}[r]
v_{21} \\
v_{22}
\end{pmatrix*}
=
\begin{pmatrix*}[r]
0 \\
0
\end{pmatrix*}
\Rightarrow
v_2 =
\frac{1}{\sqrt{2}}
\begin{pmatrix*}[r]
1 \\
-1
\end{pmatrix*}
\mbox{ ist eine Lösung. }
\end{equation}
Weiterhin gilt $v_1^Tv_2 = 0$ und $||v_1||_2 = ||v_2||_2 = 1$.


# Eigenvektoren und Eigenwerte
Beispiel (fortgeführt)
\vspace{3mm}
\footnotesize
```{r}
# Matrixdefinition
A = matrix(c(2,1,
             1,2),
           nrow  = 2,
           byrow = TRUE)

# Eigenanalyse
eigen(A)
```

# 
\vfill
\setstretch{2.5}
\Large
Eigenvektoren und Eigenwerte

**Orthonormalzerlegung**

Singulärwertzerlegung

Vektorkoordinatentransformation

Selbstkontrollfragen
\vfill 


# Orthonormalzerlegung
\small
\begin{theorem}[Eigenwerte und Eigenvektoren symmetrischer Matrizen]
\justifying
\normalfont
Eine symmetrische Matrix $S \in \mathbb{R}^{m \times m}$ hat $m$ verschiedene
Eigenwerte $\lambda_1,...,\lambda_m$ mit zugehörigen orthogonalen
Eigenvektoren $q_1,...,q_m \in \mathbb{R}^m$.
\end{theorem}
\vspace{-2mm}
\footnotesize
Bemerkungen
\vspace{-1mm}

* Das Theorem ist eine Konsequenz aus dem Spektralsatz der Linearen Algebra.
* Ein vollständiger Beweis findet sich in @strang_2009, Section 6.4.

\vspace{-2mm}
\underline{Teilbeweis}

Wir setzen die Tatsache, dass $S$ $m$ verschiedene Eigenwerte hat, als gegeben
voraus und zeigen lediglich,  dass die Eigenvektoren von $S$ orthogonal sind.
Ohne Beschränkung der Allgemeinheit seien also $\lambda_i$ und $\lambda_j$ mit
$1 \le i,j \le m$ und $\lambda_i \neq \lambda_j$ zwei der $m$ verschiedenen Eigenwerte
von $S$ mit zugehörigen Eigenvektoren $q_i$ und $q_j$, respektive. Dann ergibt sich
\begin{equation}
Sq_i = \lambda_i q_i
\Leftrightarrow
(Sq_i)^T = (\lambda_i q_i)^T
\Leftrightarrow
q_i^T S = q_i^T \lambda_i
\Leftrightarrow
q_i^T Sq_j = \lambda_i q_i^Tq_j.
\end{equation}
Ähnlicherweise gilt
\begin{equation}
Sq_j = \lambda_j q_j
\Leftrightarrow
q_i^T Sq_j = \lambda_j q_i^Tq_j.
\end{equation}
Also folgt
\begin{equation}
\lambda_i q_i^Tq_j
=
\lambda_j q_i^Tq_j
\mbox{ mit } q_i \neq 0, q_j \neq 0, \mbox{ und } \lambda_i \neq \lambda_j
\end{equation}
und damit die Orthogonalität $q_i^Tq_j = 0$.
$\hfill \Box$

# Orthonormalzerlegung
\small
\begin{theorem}[Orthonormale Zerlegung einer symmetrischen Matrix]
\normalfont
$S \in \mathbb{R}^{m \times m}$ sei eine symmetrische Matrix. Dann kannn $S$
geschrieben werden als
\begin{equation}
S = Q \Lambda Q^T,
\end{equation}
wobei $Q \in \mathbb{R}^{m \times m}$ eine orthogonale Matrix ist und
$\Lambda \in \mathbb{R}^{m\times m}$ eine Diagonalmatrix ist.
\end{theorem}
\vspace{-2mm}

\footnotesize
\underline{Beweis}

Weil $S$ symmetrisch ist, hat sie $m$ verschiedene Eigenwerte $\lambda_i, i = 1,...,m$
und $m$ zugehörige orthogonale Eigenvektoren $q_i, i = 1,...,m$, so dass
\begin{equation}
Sq_i = \lambda_i q_i \mbox{ für } i = 1,...,m.
\end{equation}
Mit den Definitionen
\begin{equation}
Q :=
\begin{pmatrix*}[r]
q_1 & q_2 & \cdots & q_m
\end{pmatrix*}
\mbox{ und }
\Lambda :=
\mbox{diag}\begin{pmatrix*}[r]
\lambda_1,\lambda_2,...,\lambda_m
\end{pmatrix*},
\end{equation}
folgt dann
\begin{equation}
SQ = \Lambda Q
\Leftrightarrow
SQ = Q\Lambda.
\end{equation}
Rechtseitige Multiplikation mit $Q^T$ ergibt dann
\begin{equation}
SQQ^T = Q \Lambda Q^T
\Leftrightarrow SI_m = Q \Lambda Q^T
\Leftrightarrow S    = Q \Lambda Q^T
\end{equation}
und damit ist alles gezeigt.
$\hfill \Box$

# Orthonormalzerlegung
\setstretch{1.6}

Beispiel (fortgeführt)

\footnotesize
Für
\begin{equation}
Q := \begin{pmatrix*}[r]
v_1 & v_2
\end{pmatrix*}
\mbox{ and }
\Lambda = \mbox{diag}(\lambda_1,\lambda_2)
\end{equation}
ergibt sich
\begin{align*}
Q\Lambda Q^T
& =
\begin{pmatrix*}[r]
v_1 & v_2
\end{pmatrix*}
\mbox{diag}(\lambda_1,\lambda_2)
\begin{pmatrix*}[r]
v_1 & v_2
\end{pmatrix*}^T \\
& =
\frac{1}{\sqrt{2}}
\begin{pmatrix*}[r]
1 &  1\\
1 & -1
\end{pmatrix*}
\begin{pmatrix*}[r]
3 & 0 \\
0 & 1
\end{pmatrix*}
\begin{pmatrix*}[r]
1 &  1 \\
1 & -1
\end{pmatrix*}  \\
& =
\frac{1}{\sqrt{2}}
\begin{pmatrix*}[r]
3 & 1 \\
3 & -1
\end{pmatrix*}
\begin{pmatrix*}[r]
1 &  1 \\
1 & -1
\end{pmatrix*} \\
& =
\frac{1}{\sqrt{2}}
\begin{pmatrix*}[r]
4 & 2 \\
2 & 4
\end{pmatrix*} \\
& =
\begin{pmatrix*}[r]
2 & 1 \\
1 & 2
\end{pmatrix*} \\
& = A
\end{align*}


# 
\vfill
\setstretch{2.5}
\Large
Eigenvektoren und Eigenwerte

Orthonormalzerlegung

**Singulärwertzerlegung**

Vektorkoordinatentransformation

Selbstkontrollfragen
\vfill 


# Singulärwertzerlegung
\small
\begin{definition}[Singulärwertzerlegung]
\justifying
$X \in \mathbb{R}^{m \times n}$ sei eine Matrix. Dann heißt die Zerlegung
\begin{equation}
X = USV^T,
\end{equation}
wobei $U \in \mathbb{R}^{m \times m}$ eine orthogonale Matrix ist, $S \in \mathbb{R}^{m \times n}$
eine Diagonalmatrix ist und $V \in \mathbb{R}^{n \times n}$ eine orthogonale Matrix ist,
\textit{Singulärwertzerlegung (Singular Value Decomposition (SVD))} von $X$. Die
Diagonalelemente von  $S$ heißen die  \textit{Singulärwerte} von $X$.
\end{definition}

Bemerkungen
\begin{itemize}
\item Die Existenz der Singulärwertzerlegung folgt aus dem Spektralsatz der Linearen Algebra.
\item Singulärwertzerlegungen können in R mit  svd()  berechnet werden.
\end{itemize}


# Singulärwertzerlegung
\small
\begin{theorem}[Singulärwertzerlegung und Eigenanalyse]
\justifying
\normalfont
$X \in \mathbb{R}^{m \times n}$ sei eine Matrix und
\begin{equation}
X = USV^T
\end{equation}
sei ihre Singulärwertzerlegung. Dann gilt:
\begin{itemize}
\item Die Spalten von $U$ sind die Eigenvektoren von $XX^T$,
\item die Spalten von $V$ sind die Eigenvektoren von $X^TX$ und
\item die entsprechenden Singulärwerte sind die Quadratwurzeln der zugehörigen Eigenwerte.
\end{itemize}
\end{theorem}

Bemerkung

* Singulärwertzerlegung und Eigenanalyse sind eng verwandt.

# Singulärwertzerlegung
\footnotesize
\underline{Beweis}
\vspace{1mm}

Wir halten zunächst fest, dass mit
\begin{equation}
\left(XX^T\right)^T = XX^T \mbox{ and } \left(X^TX\right)^T = X^TX,
\end{equation}
$XX^T$ und $X^TX$ symmetrische Matrizen sind und somit Orthornomalzerlegungen haben.
Wir halten weiterhin fest, dass mit der Definition der Singulärwertzerlegung gelten,
dass sowohl
\begin{equation}
XX^T
= USV^T \left(U\Sigma V^T\right)^T
= USV^T V S^T U^T
= USSU^T
= U\Lambda U^T
\end{equation}
als auch
\begin{equation}
X^TX
= \left(USV^T\right)^T USV^T
= VS^T UUS^T V^T
= V\Lambda V^T
\end{equation}
ist, wobei wir $\Lambda := SS$ definiert haben. Weil das Produkt von Diagonalmatrizen
wieder eine Diagonalmatrix ist, ist $\Lambda$ eine Diagonalmatrix und per Definition
sind $U$ und $V$ orthogonale Matrizen. Wir haben also $XX^T$ und $X^TX$
in Form der Orthonormalzerlegungen
\begin{equation}
XX^T = U \Lambda U^T  \mbox{ and } X^TX = V \Lambda V^T
\end{equation}
geschrieben und damit ist alles gezeigt.
$\hfill \Box$


# 
\vfill
\setstretch{2.5}
\Large
Eigenvektoren und Eigenwerte

Orthonormalzerlegung

Singulärwertzerlegung

**Vektorkoordinatentransformation**

Selbstkontrollfragen
\vfill 

# Vektorkoordinatentransformation
\small
\textcolor{darkblue}{Im Folgenden wichtige Begriffe}

*Euklidischer Vektorraum.* Das Tupel $\left((\mathbb{R}^m, +, \cdot), \langle \rangle \right)$
aus dem reellen Vektorraum $(\mathbb{R}^m, +, \cdot)$ und dem Skalarprodukt
$\langle \rangle$ auf $\mathbb{R}^m$ heißt \textit{reeller kanonischer Euklidischer Vektorraum}.
\vspace{2mm}

*Basis.* $V$ sei ein Vektorraum und es sei $B \subseteq V$. Dann heißt $B$ eine
\textit{Basis von $V$}, wenn  die Vektoren in $B$ linear unabhängig sind und
die Vektoren in $B$ den Vektorraum $V$ aufspannen.
\vspace{2mm}

*Basisdarstellung und Koordinaten.* $B := \{b_1,...,b_m\}$ sei eine Basis
eines $m$-dimensionalen Vektorraumes $V$ und es sei $x \in V$. Dann heißt die
Linearkombination $x = \sum_{i = 1}^m a_i b_i$ die \textit{Darstellung von
$x$ bezüglich der Basis $B$} und die Koeffizienten $a_1,...,a_m$ heißen die
\textit{Koordinaten von $x$ bezüglich der Basis $B$}.
\vspace{2mm}

*Orthonormalbasis von $\mathbb{R}^m$.* Eine Menge von $m$ Vektoren
$q_1,...,q_m \in \mathbb{R}^m$ heißt \textit{Orthonormalbasis} von $\mathbb{R}^m$,
wenn $q_1,...,q_m$ jeweils die Länge 1 haben und wechselseitig orthogonal sind.

*Orthonormale Zerlegung einer symmetrischen Matrix*. $S \in \mathbb{R}^{m \times m}$
sei eine symmetrische Matrix. Dann kannn $S$ geschrieben werden als $S = Q \Lambda Q^T$,
wobei $Q \in \mathbb{R}^{m \times m}$ eine orthogonale Matrix ist und
$\Lambda \in \mathbb{R}^{m\times m}$ eine Diagonalmatrix ist. Dabei sind die Spalten
von $Q$ die Eigenvektoren von $S$ und die Diagonalelemente von $\Lambda$ sind die
entsprechenden Eigenwerte.


# Vektorkoordinatentransformationn
\textcolor{darkblue}{Im Folgenden wichtige Intuition}
\vspace{1mm}

```{r, echo = FALSE, out.width = "70%"}
knitr::include_graphics("4_Abbildungen/mvda_4_basen_R2.pdf")
```

\small
Bei Hauptkomponenten- und Faktorenanalysen werden aus den Koordinaten eines 
Vektors bezüglich einer Basis die Koordinaten desselben Vektors bezüglich einer 
anderen Basis berechnet.

# Vektorkoordinatentransformation
\small
\begin{definition}[Orthogonalprojektion]
$x$ und $q$ seien Vektoren im Euklidischen Vektorraum $\mathbb{R}^m$. Dann ist die
\textit{Orthogonalprojektion von $x$ auf $q$} definiert als der Vektor
\begin{equation}
\tilde{x} = aq \mbox{ mit } a := \frac{q^T x}{q^T q},
\end{equation}
wobei der Skalar $a$ \textit{Projektionsfaktor} genannt wird.
\end{definition}

\footnotesize
Bemerkungen
\begin{itemize}
\item Per definition ist  $\tilde{x} = aq$ mit $a \in \mathbb{R}$ der Punkt in Richtung von $q$ der $x$ am nähesten ist.
\item Diese minimierte Distanzeigenschaft impliziert die Orthogonalität von $q$ und $x - \tilde{x}$.
\item Die Formel von $a$ folgt direkt aus der Orthogonalität von $x - \tilde{x}$ und $q$, da gilt
\begin{equation*}
q^T(x - \tilde{x}) = 0
\Leftrightarrow
q^T(x - aq) = 0
\Leftrightarrow
q^Tx - aq^Tq = 0
\Leftrightarrow
a = \frac{q^Tx}{q^Tq}.
\end{equation*}
\item Wenn $q$ die Länge $||q|| = \sqrt{q^Tq} = 1$ hat, dann gilt $a = \frac{q^T x}{||q||^2} = q^T x$.
\end{itemize}

# Vektorkoordinatentransformation
Orthogonalprojektion

```{r, echo = FALSE, out.width = "60%"}
knitr::include_graphics("4_Abbildungen/mvda_4_orthogonale_vektorprojektion.pdf")
```

# Vektorkoordinatentransformation
\footnotesize
\begin{theorem}[Vektorkoordinaten bezüglich einer Orthogonalbasis]
\justifying
\normalfont
Es sei $x \in \mathbb{R}^m$ und es sei $B := \{q_1,...,q_m\}$ eine Orthonormalbasis
von von $\mathbb{R}^m$. Dann ergeben sich für $i = 1,...,m$ die Koordinaten $a_i$ in
der Basisdarstellung von $x$ bezüglich $B$ als die Projektionsfaktoren
\begin{equation}
a_i = x^T q_i
\end{equation}
in der Orthogonalprojektion von $x$ auf $q_i$. Äquivalent ist die
Basisdarstellung von $x$ bezüglich $B$ gegeben durch
\begin{equation}
x = \sum_{i=1}^m (x^T q_i)q_i.
\end{equation}
\end{theorem}

\footnotesize
\underline{Beweis}

Für $i = 1,...,m$ gilt
\begin{equation}
x = \sum_{j=1}^m a_j q_j
\Leftrightarrow
q_i^T x = q_i^T \sum_{j=1}^m a_j q_j
\Leftrightarrow
q_i^T x = \sum_{j=1}^m a_j q_i^Tq_j
\Leftrightarrow
q_i^T x = a_i
\Leftrightarrow
a_i = x^T q_i.
\end{equation}
$\hfill \Box$

# Vektorkoordinatentransformation
\footnotesize
\begin{theorem}[Vektorkoordinatentransformation]
\justifying
\normalfont
$B_v := \{v_1,...,v_m\}$ und $B_w := \{w_1,...,w_m\}$ seien zwei Orthonormalbasen
eines Vektorraums. $A \in \mathbb{R}^{m \times m}$ sei die Matrix, die durch die
spaltenweise Konkatenation der Koordinaten der Vektoren in $B_w$ in der Basisdarstellung
bezüglich der Basis $B_v$ ergibt. Dann können die Koordinaten $x_i, i = 1,...,m$
eines Vektors $x$ bezüglich der Basis $B_v$ in die Koordinaten $\tilde{x}_1,...,\tilde{x}_m$
des Vektors bezüglich der Basis $B_w$  durch
\begin{equation}
\tilde{x} = A^T x
\end{equation}
transformiert werden. Analog können die Koordinaten $\tilde{y}_1,...,\tilde{y}_m$
des Vektors hinsichtlich der Basis $B_w$ in die Koordinaten $y_1,...,y_m$ des
Vektors hinsichtlich $B_v$ durch
\begin{equation}
x = A \tilde{x}.
\end{equation}
transformiert werden.
\end{theorem}

\footnotesize
Bemerkungen

* \justifying Das Theorem erlaubt die Berechnung von Vektorkoordinaten bezüglich einer anderen Orthonormalbasis.
* Für die Berechnung muss zunächst die Matrix $A$ gebildet und dann (nur) entsprechend multipliziert werden.
* Wir verzichten auf einen Beweis und demonstrieren das Theorem an einem Beispiel.

\small
\center
\textcolor{darkblue}{Ein Vektor wird hier als  fester Punkt in $\mathbb{R}^m$ betrachtet; die Komponenten (Zahlen) des Vektors werden dagegen nur als Koordinaten bezüglich einer spezifischen Basis interpretiert!}

# Vektorkoordinatentransformation
Beispiel
\vspace{3mm}

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("4_Abbildungen/mvda_4_vektorkoordinatentransformation.pdf")
```
\center
\vspace{1mm}
\small
Man beachte, dass $x$ and $\tilde{x}$ am selben Ort in $\mathbb{R}^2$ liegen!


# Vektorkoordinatentransformation
\setstretch{1.2}
\vspace{2mm}
Beispiel
\vspace{1mm}

\footnotesize
Wir nehmen an, dass wir die Koordinaten von $x = (1/3, 2/3)^T \in \mathbb{R}^2$
hinsichtlich der kanonischen Orthonormalbasis $B_v :=\{e_1,e_2\}$ in die Koordinaten
bezüglich der Basis
\begin{equation}
B_w :=
\left\lbrace
\begin{pmatrix}
\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}
\end{pmatrix},
\begin{pmatrix}
-\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}
\end{pmatrix}
\right\rbrace
\end{equation}
transformieren wollen. Die Basisdarstellungen der in Vektoren $B_w$ bezüglich der Basisvektoren in $B_v$ sind
\begin{equation}
\begin{pmatrix}
\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}
\end{pmatrix}
=
a_{11}
\begin{pmatrix}
1 \\ 0
\end{pmatrix}
+
a_{21}
\begin{pmatrix}
0 \\ 1
\end{pmatrix}
\mbox{ and }
\begin{pmatrix}
-\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}
\end{pmatrix}
=
a_{12}
\begin{pmatrix}
1 \\ 0
\end{pmatrix}
+
a_{22}
\begin{pmatrix}
0 \\ 1
\end{pmatrix}.
\end{equation}
Die Projektionsfaktoren der Orthogonalprojektionen der Vektoren in $B_w$ auf die
Vektoren in $B_v$ sind
\begin{equation}
a_{11} =  \frac{1}{\sqrt{2}},
a_{21} =  \frac{1}{\sqrt{2}},
a_{12} = -\frac{1}{\sqrt{2}},
a_{22} =  \frac{1}{\sqrt{2}}.
\end{equation}
Die Transformationsmatrix $A \in \mathbb{R}^{m \times m}$ in obigem Theorem ergibt sich also zu
\begin{equation}
A =
\begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{pmatrix}
=
\begin{pmatrix}
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}} &  \frac{1}{\sqrt{2}} \\
\end{pmatrix}.
\end{equation}
Die Vektorkoordinatentransformation von $x \in \mathbb{R}^2$ ergibt sich also zu
\begin{equation}
\tilde{x}
= A^T x
= \begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} 	\\
-\frac{1}{\sqrt{2}} &  \frac{1}{\sqrt{2}} 	\\
\end{pmatrix}
\begin{pmatrix}
\frac{1}{3} \\
\frac{2}{3}
\end{pmatrix}
\approx
\begin{pmatrix}
0.70 \\
0.23
\end{pmatrix}.
\end{equation}

# 
\vfill
\setstretch{2.5}
\Large
Eigenvektoren und Eigenwerte

Orthonormalzerlegung

Singulärwertzerlegung

Vektorkoordinatentransformation

**Selbstkontrollfragen**
\vfill 

# Selbstkontrollfragen
\footnotesize
\setstretch{2}
\begin{enumerate}
\item Geben Sie die Definition eines Eigenvektors und eines Eigenwertes einer quadratischen Matrix wieder.
\item Geben Sie das Theorem zur Bestimmung von Eigenwerten und Eigenvektoren wieder.
\item Geben Sie das Theorem zu den Eigenwerten und Eigenvektoren symmetrischer Matrizen wieder.
\item Geben Sie das Theorem zur orthonormalen Zerlegung einer symmetrischen Matrix wieder.
\item Geben Sie die Definition einer Singulärwertzerlegung wieder.
\item Geben Sie das Theorem zum Zusammenhang von Singulärwertzerlegung und Eigenanalyse wieder.
\item Definieren Sie den Begriff Orthogonalprojektion.
\item Geben Sie das Theorem zu Vektorkoordinaten bezüglich einer Orthogonalbasis wieder.
\item Geben Sie das Vektorkoordinatentransformationstheorem wieder.
\item Erläutern Sie das Vektorkoordinatentransformationstheorem.
\end{enumerate}

# References
\footnotesize
