---
fontsize: 8pt
bibliography: 4_Referenzen.bib
citation_package: natbib
output:
  beamer_presentation:
    keep_tex: true
    includes:
      in_header: 4_Header.tex
---


```{r, include = F}
source("4_R_common.R")
```

#  {.plain}
\center
```{r, echo = FALSE, out.width = "20%"}
knitr::include_graphics("4_Abbildungen/mvda_4_otto.png")
```

\vspace{2mm}

\Huge
Multivariate Datenanalyse
\vspace{6mm}

\large
MSc Psychologie WiSe 2022/23

\vspace{6mm}
\large
Prof. Dr. Dirk Ostwald

#  {.plain}

\vfill
\center
\huge
\textcolor{black}{(4) Eigenanalyse}
\vfill

# 
\vfill
\setstretch{3}
\Large
Eigenvektoren und Eigenwerte

Orthonormalzerlegung 

Singulärwertzerlegung

Selbstkontrollfragen
\vfill 

# 
\vfill
\setstretch{3}
\Large
**Eigenvektoren und Eigenwerte**

Orthonormalzerlegung 

Singulärwertzerlegung

Selbstkontrollfragen
\vfill 

# Eigenvektoren und Eigenwerte
\small
\begin{definition}[Eigenvektor, Eigenwert]
$A \in \mathbb{R}^{m \times m}$ sei eine quadratische Matrix. Dann heißt jeder
Vektor $v \in \mathbb{R}^m, v \neq 0_m$, für den gilt, dass
\begin{equation}
Av  = \lambda v
\end{equation}
mit $\lambda \in \mathbb{R}$ ein \textit{Eigenvektor} von $A$. $\lambda$
heißt zugehöriger \textit{Eigenwert} von $A$.
\end{definition}
\footnotesize
Bemerkungen

* Ein Eigenvektor $v$ von $A$ wird durch $A$ mit einem Faktor $\lambda$ verlängert.
* Jeder Eigenvektor hat einen zugehörigen Eigenwert.
* Die Eigenwerte  verschiedener Eigenvektor können identisch sein.

# Eigenvektoren und Eigenwerte
\small
\begin{theorem}[Multiplikativität von Eigenvektoren]
\justifying
\normalfont
$A \in \mathbb{R}^{m \times m}$ sei eine quadratische Matrix. Wenn $v \in \mathbb{R}^m$
Eigenvektor von $A$ mit Eigenwert $\lambda \in \mathbb{R}$ ist, dann ist für $c \in \mathbb{R}$ 
auch $cv \in \mathbb{R}^m $ Eigenvektor von $A$ und zwar wiederum mit Eigenwert $\lambda \in \mathbb{R}$.
\end{theorem}

\footnotesize
\underline{Beweis}

Es gilt
\begin{equation}
Av = \lambda v   \Leftrightarrow 
cAv = c\lambda v \Leftrightarrow 
A(cv) = \lambda(cv)
\end{equation}
Also ist $cv$ ein Eigenvektor von $A$ mit Eigenwert $\lambda$.

$\hfill\Box$

\normalsize
**Konvention**

Wir betrachten im Folgenden nur Eigenvektoren mit $\Vert v \Vert = 1$.


# Eigenvektoren und Eigenwerte {.t}
\vspace{2mm}
\textcolor{darkblue}{Visualisierung eines Eigenvektors}
\footnotesize
\vspace{2mm}

Für $A := \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$ ist 
$v := \frac{1}{\sqrt{2}} \begin{pmatrix} 1 \\ 1 \end{pmatrix}$ 
Eigenvektor zum Eigenwert $\lambda = 3, w := \begin{pmatrix} 1 \\ 0 \end{pmatrix}$ ist kein Eigenvektor.

\center
```{r, eval = F, echo = F}
library(latex2exp)
graphics.off()
fdir        =  file.path(getwd(), "4_Abbildungen")
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = .95,
cex.main    = 1.2)

# Definitionen
A           = matrix(c(2,1,
                       1,2),
                     byrow = TRUE,
                     nrow = 2) 
v           = matrix(c(1/sqrt(2),
                       1/sqrt(2)),
                       nrow = 2)
w           = matrix(c(1,
                       0),
                       nrow = 2)
Av          = A %*% v
Aw          = A %*% w

# Punktperspektive
plot(
NULL,
xlab        = TeX("$x_1$"),
ylab        = TeX("$x_2$"),
xlim        = c(0,3),
ylim        = c(0,3))
grid()
points(
c(v[1], w[1], Av[1], Aw[1]),
c(v[2], w[2], Av[2], Aw[2]),
pch = 19,
xpd = TRUE)
text(1/sqrt(2), 0.9, TeX("$v$"))
text(1        , 0.2, TeX("$w$"))
text(3/sqrt(2), 2.4, TeX("$Av = \\lambda v$"))
text(2        , 1.3, TeX("$Aw \\neq \\lambda w$"))

# Pfeilperspektive
arrows(
x0          = c(0,0,0,0),
y0          = c(0,0,0,0),
x1          = c(v[1],w[1],Av[1],Aw[1]),
y1          = c(v[2],w[2],Av[2],Aw[2]),
angle       = 20,
length      = .1,
lwd         = 1.5,
col         = c("black", "gray60", "black", "gray60"),
xpd         = TRUE)


# export to pdf
dev.copy2pdf(                                                                    # export to PDF
             file   = file.path(fdir, "mvda_4_eigenvektor.pdf"),                 # filename
             width  = 4,                                                         # PDF width
             height = 4                                                          # PDF height
             )

```

    
```{r, echo = FALSE, out.width = "55%"}  
knitr::include_graphics("4_Abbildungen/mvda_4_eigenvektor.pdf") 
```


# Eigenvektoren und Eigenwerte
\small
\begin{theorem}[Bestimmung von Eigenwerten und Eigenvektoren]
\normalfont
\justifying
$A \in \mathbb{R}^{m \times m}$ sei eine quadratische Matrix. Dann ergeben sich
die Eigenwerte von $A$ als die Nullstellen des \textit{charakteristischen Polynoms}
\begin{equation}
\chi_A(\lambda) := \det(A - \lambda I_m) 
\end{equation}
von $A$. Weiterhin seien $\lambda_i^*, i = 1,2,...$ die auf diese Weise bestimmten
Eigenwerte von $A$. Die entsprechenden Eigenvektoren $v_i, i = 1,2,...$ von $A$
können dann durch Lösen der linearen Gleichungssysteme
\begin{equation}
(A - \lambda_i^* I_m)v_i = 0_m \mbox{ für } i = 1,2,...
\end{equation}
bestimmt werden.
\end{theorem}

\footnotesize
Bemerkungen

* Für kleine Matrizen mit $m \le 3$ können Eigenwerte und Eigenvektoren manuell bestimmt werden.
* Bei großen Matrizen werden Eigenwerte und Eigenvektor im Allgemeinen numerisch bestimmt.
* R's `eigen()`, Scipy's `linalg.eig()`, Matlab's `eig()`.

# Eigenvektoren und Eigenwerte
\setstretch{1.2}
\footnotesize
\underline{Beweis}

\noindent (1) Bestimmen von Eigenwerten

Wir halten zunächst fest, dass mit der Definition
von Eigenvektoren und
Eigenwerten gilt, dass
\begin{equation}
Av = \lambda v
\Leftrightarrow Av - \lambda v = 0_m
\Leftrightarrow (A - \lambda I_m)v = 0_m.
\end{equation}
Für den Eigenwert $\lambda$ wird der Eigenvektor $v$ also durch $(A - \lambda I_m)$
auf den Nullvektor $0_m$ abgebildet. Weil aber per Definition $v \neq 0_m$ gilt,
ist die Matrix $(A - \lambda I_m)$ somit nicht invertierbar: sowohl der Nullvektor
als auch $v$ werden durch $A$ auf $0_m$ abgebildet, die Abbildung
\begin{equation}
f : \mathbb{R}^m \to \mathbb{R}^m, x \mapsto (A - \lambda I_m)x
\end{equation}
ist also nicht bijektiv, und $(A - \lambda I_m)^{-1}$ kann nicht existieren.
Die Tatsache, dass $(A - \lambda I_m)$ nicht invertierbar ist, ist aber
äquivalent dazu, dass die Determinante von $(A -\lambda I_m)$ Null ist. Also ist
\begin{equation}
\chi_A(\lambda) = \det(A - \lambda I_m) = 0
\end{equation}
notwendige und hinreichende Bedingung dafür, dass $\lambda$ ein Eigenwert von
$A$ ist.

\noindent (2) Bestimmen von Eigenvektoren

Es sei $\lambda_i^*$ ein Eigenwert von $A$. Dann
gilt mit den obigen Überlegungen, dass Auflösen von
\begin{equation}
(A - \lambda_i^* I_m)v_i^* = 0_m
\end{equation}
nach $v_i^*$ einen Eigenvektor zum Eigenwert $\lambda^*$  ergibt.

$\hfill \Box$

# Eigenvektoren und Eigenwerte
\setstretch{1.6}
Beispiel  

\footnotesize
Es sei
\begin{equation}
A :=
\begin{pmatrix*}[r]
2 & 1 \\
1 & 2\end{pmatrix*}
\end{equation}
Wir wollen die Eigenwerte und Eigenvektoren von $A$ bestimmen.
\vspace{1mm}


\small
\noindent (1) Berechnen von Eigenwerten
\footnotesize

Die Eigenwerte von $A$ sind die Nullstellen des charakteristischen Polynoms von $A$.

Das charakteristische Polynom von $A$ ergibt als
\begin{equation}
\chi_A(\lambda)
=
\det\left(
\begin{pmatrix*}[r]
2 & 1 \\
1 & 2\end{pmatrix*}
-
\begin{pmatrix*}[r]
\lambda & 0 \\
0 		& \lambda
\end{pmatrix*}
\right)
=
\det
\begin{pmatrix*}[r]
2 - \lambda & 1 \\
1 & 2 - \lambda
\end{pmatrix*}
= (2 - \lambda)^2 - 1.
\end{equation}
Nullsetzen und Auflösen nach $\lambda$ ergibt mit der [\textcolor{blue}{pq-Formel}](https://de.wikipedia.org/wiki/Quadratische_Gleichung)
\begin{equation}
(2 - \lambda)^2 - 1 = 0 \Rightarrow \lambda_1 = 3, \lambda_2 = 1.
\end{equation}
Die Eigenwerte von $A$ sind also $\lambda_1 = 3$ und $\lambda_2 = 1$.


# Eigenvektoren und Eigenwerte
\setstretch{1.6}
Beispiel (fortgeführt)

\small
\noindent (2) Berechnen von Eigenvektoren

\footnotesize
Die Eigenvektoren zu den Eigenwerten $\lambda_1 = 3$ und $\lambda_2 = 1$ ergeben sich
durch Lösen der linearen Gleichungssysteme
\begin{equation}
(A - \lambda_i I_2)v_i = 0_2 \mbox{ für } i = 1,2.
\end{equation}
Für $\lambda_1 = 3$ ergibt sich
\begin{equation}
(A - 3I_2)v_1 = 0_2
\Leftrightarrow
\begin{pmatrix*}[r]
-1 & 1 \\
 1 & -1
\end{pmatrix*}
\begin{pmatrix*}[r]
v_{1_1} \\
v_{1_2}
\end{pmatrix*}
=
\begin{pmatrix*}[r]
0 \\
0
\end{pmatrix*}
\Rightarrow
v_1 =
\frac{1}{\sqrt{2}}
\begin{pmatrix*}[r]
1 \\
1
\end{pmatrix*}
\mbox{ ist eine Lösung. }
\end{equation}
Fpr $\lambda_2 = 1$ ergibt sich
\begin{equation}
(A - 1I_2)v_2 = 0_2
\Leftrightarrow
\begin{pmatrix*}[r]
1 & 1 \\
1 & 1
\end{pmatrix*}
\begin{pmatrix*}[r]
v_{2_1} \\
v_{2_2}
\end{pmatrix*}
=
\begin{pmatrix*}[r]
0 \\
0
\end{pmatrix*}
\Rightarrow
v_2 =
\frac{1}{\sqrt{2}}
\begin{pmatrix*}[r]
-1 \\
 1
\end{pmatrix*}
\mbox{ ist eine Lösung. }
\end{equation}
Weiterhin gilt $v_1^Tv_2 = 0$ und $||v_1|| = ||v_2|| = 1$.


# Eigenvektoren und Eigenwerte
Beispiel (fortgeführt)
\vspace{3mm}
\footnotesize
```{r}
# Matrixdefinition
A = matrix(c(2,1,
             1,2),
           nrow  = 2,
           byrow = TRUE)

# Eigenanalyse
eigen(A)
```

# 
\vfill
\setstretch{3}
\Large
Eigenvektoren und Eigenwerte

**Orthonormalzerlegung**

Singulärwertzerlegung

Selbstkontrollfragen
\vfill 


# Orthonormalzerlegung
\small
\begin{theorem}[Eigenwerte und Eigenvektoren symmetrischer Matrizen]
\justifying
\normalfont
$S \in \mathbb{R}^{m \times m}$ sei eine symmetrische Matrix. Dann gelten
\begin{itemize}
\item[(1)] Die Eigenwerte von $S$ sind reell.
\item[(2)] Die Eigenvektoren zu je zwei verschiedenen Eigenwerten von $S$ sind orthogonal. 
\end{itemize}
\end{theorem}
\footnotesize

Bemerkung 

* \justifying In nachfolgendem Beweis setzen wir die Tatsache dass eine symmetrische $m$ 
  reelle Eigenwerte hat als gegeben voraus und zeigen lediglich, dass die 
  Eigenvektoren zu je zwei verschiedenen Eigenwerten einer symmetrischen Matrix 
  orthogonal sind. Ein vollständiger Beweis des Theorems findet sich in @strang_2009, Kapitel 6.4.
* Da wir als Eigenvektoren nur Eigenvektoren der Länge 1 betrachten, sind die hier angesprochenen
  orthogonalen Eigenvektoren insbesondere auch orthonormal.

# Orthonormalzerlegung
\footnotesize
\underline{Beweis}

Ohne Beschränkung der Allgemeinheit seien  $\lambda_i$ und $\lambda_j$ mit
$1 \le i,j \le m$ und $\lambda_i \neq \lambda_j$ zwei verschiedenen Eigenwerte
von $S$ mit zugehörigen Eigenvektoren $q_i$ und $q_j$, respektive. Dann ergibt sich
wie unten gezeigt, dass
\begin{equation}
\lambda_i q_i^Tq_j = \lambda_j q_i^Tq_j.
\end{equation}
Mit $q_i \neq 0_m, q_j \neq 0_m$  und $\lambda_i \neq \lambda_j$ folgt damit $q_i^Tq_j = 0$, weil
weil es keine andere Zahl $c$ als die Null gibt, für die bei $a,b\in \mathbb{R}$ und $a \neq b$ gilt, 
dass
\begin{equation}
ac = bc.
\end{equation}
Um 
\begin{equation}
\lambda_i q_i^Tq_j = \lambda_j q_i^Tq_j.
\end{equation}
zu zeigen, halten wir zunächst fest, dass 
\begin{equation}
                 Sq_i       = \lambda_i q_i      
\Leftrightarrow (Sq_i)^T    = (\lambda_i q_i)^T  
\Leftrightarrow q_i^TS^T    = q_i^T \lambda_i^T  
\Leftrightarrow q_i^T S     = q_i^T \lambda_i   
\Leftrightarrow q_i^T Sq_j  = \lambda_i q_i^Tq_j 
\end{equation}
und 
\begin{equation}
                     Sq_j = \lambda_j q_j
\Leftrightarrow q_i^TSq_j = \lambda_j q_i^Tq_j
\end{equation}
gelten. Sowohl $\lambda_i q_i^Tq_j$ als auch $\lambda_j q_i^Tq_j$ sind also 
mit $q_i^T Sq_j$ und damit auch miteinander identisch.

$\hfill \Box$

# Orthonormalzerlegung
\setstretch{1.2}
\footnotesize
\begin{theorem}[Orthonormale Zerlegung einer symmetrischen Matrix]
\normalfont
$S \in \mathbb{R}^{m \times m}$ sei eine symmetrische Matrix mit $m$ verschiedenen
Eigenwerten. Dann kannn $S$
geschrieben werden als
\begin{equation}
S = Q \Lambda Q^T,
\end{equation}
wobei $Q \in \mathbb{R}^{m \times m}$ eine orthogonale Matrix ist und
$\Lambda \in \mathbb{R}^{m\times m}$ eine Diagonalmatrix ist.
\end{theorem}
\vspace{-1mm}
Bemerkungen
\vspace{-1mm}

* $S = Q \Lambda Q^T$ heißt auch *Diagonalisierung* von $S$. 
* Wie im Beweis gezeigt wählt man als Spalten von $Q$ die orthonormalen Eigenvektoren von $S$.
* Wie im Beweis gezeigt wählt man als Diagonalelemente von $\Lambda$ die zugehörigen Eigenwerte von $S$.


\footnotesize
\underline{Beweis}

Es seien $\lambda_1,...,\lambda_m$ die Eigenwerte von $S$ und $q_1,...,q_m$ die 
zugehörigen orthonormalen Eigenvektoren. Mit 
\begin{equation}
Q :=
\begin{pmatrix*}[r]
q_1 & q_2 & \cdots & q_m
\end{pmatrix*}
\in \mathbb{R}^{m \times m}
\mbox{ und }
\Lambda :=
\mbox{diag}\begin{pmatrix*}[r]
\lambda_1,\lambda_2,...,\lambda_m
\end{pmatrix*}
\in \mathbb{R}^{m \times m},
\end{equation}
folgt dann mit den Definitionen von Eigenwerten und Eigenvektoren, dass
\begin{equation}
Sq_i = \lambda_i q_i \mbox{ für } i = 1,...,m
\Leftrightarrow
Sq_i = q_i \lambda_i \mbox{ für } i = 1,...,m
\Leftrightarrow
SQ = Q\Lambda.
\end{equation}
Rechtseitige Multiplikation mit $Q^T$ ergibt dann mit $QQ^T = I_m$, dass
\begin{equation}
SQQ^T = Q \Lambda Q^T
\Leftrightarrow SI_m = Q \Lambda Q^T
\Leftrightarrow S    = Q \Lambda Q^T.
\end{equation}
$\hfill \Box$

# Orthonormalzerlegung
\setstretch{1.12}
\small
\vspace{2mm}
Beispiel (fortgeführt)

\footnotesize
Für die symmetrische Matrix
\begin{equation}
A = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
\end{equation}
mit den oben bestimmten Eigenwerten $\lambda_1 = 3,\lambda_2 = 1$ und zugehörigen orthonormalen 
Eigenvektoren 
\begin{equation}
v_1 = \frac{1}{\sqrt{2}}
\begin{pmatrix*}[r]
1 \\
1 
\end{pmatrix*},
v_2 = \frac{1}{\sqrt{2}}
\begin{pmatrix*}[r]
-1 \\
1 
\end{pmatrix*}
\end{equation}
seien
\begin{equation}
Q := \begin{pmatrix*}[r]
v_1 & v_2
\end{pmatrix*}
\mbox{ und }
\Lambda = \mbox{diag}(\lambda_1,\lambda_2).
\end{equation}
Dann ergibt sich
\begin{align*}
Q\Lambda Q^T
& =
\begin{pmatrix*}[r]
v_1 & v_2
\end{pmatrix*}
\mbox{diag}(\lambda_1,\lambda_2)
\begin{pmatrix*}[r]
v_1 & v_2
\end{pmatrix*}^T \\
& =
\left(
\frac{1}{\sqrt{2}}
\begin{pmatrix*}[r]
1 & -1\\
1 &  1
\end{pmatrix*}
\begin{pmatrix*}[r]
3 & 0 \\
0 & 1
\end{pmatrix*}
\right)
\left(
\frac{1}{\sqrt{2}}
\begin{pmatrix*}[r]
 1 &  1 \\
-1 &  1
\end{pmatrix*}
\right)
\\
& =
\left(
\frac{1}{\sqrt{2}}
\begin{pmatrix*}[r]
3 & -1 \\
3 &  1
\end{pmatrix*}
\right)
\left(
\frac{1}{\sqrt{2}}
\begin{pmatrix*}[r]
 1 &  1 \\
-1 &  1
\end{pmatrix*}
\right)
\\
& =
\frac{1}{2}
\begin{pmatrix*}[r]
4 & 2 \\
2 & 4
\end{pmatrix*} \\
& =
\begin{pmatrix*}[r]
2 & 1 \\
1 & 2
\end{pmatrix*} \\
& = A.
\end{align*}


# 
\vfill
\setstretch{3}
\Large
Eigenvektoren und Eigenwerte

Orthonormalzerlegung

**Singulärwertzerlegung**



Selbstkontrollfragen
\vfill 


# Singulärwertzerlegung
\small
\begin{definition}[Singulärwertzerlegung]
\justifying
$Y \in \mathbb{R}^{m \times n}$ sei eine Matrix. Dann heißt die Zerlegung
\begin{equation}
Y = USV^T,
\end{equation}
wobei $U \in \mathbb{R}^{m \times m}$ eine orthogonale Matrix ist, $S \in \mathbb{R}^{m \times n}$
eine Diagonalmatrix ist und $V \in \mathbb{R}^{n \times n}$ eine orthogonale Matrix ist,
\textit{Singulärwertzerlegung (Singular Value Decomposition (SVD))} von $Y$. Die
Diagonalelemente von  $S$ heißen die  \textit{Singulärwerte} von $Y$.
\end{definition}

\footnotesize
Bemerkungen

* Für eine ausführliche Diskussion der Singulärwertzerlegung siehe z.B. @strang_2009, Kapitel 7.
* Singulärwertzerlegungen können in R mit `svd()`  berechnet werden.


# Singulärwertzerlegung
\small
\begin{theorem}[Singulärwertzerlegung und Eigenanalyse]
\justifying
\normalfont
$Y \in \mathbb{R}^{m \times n}$ sei eine Matrix und
\begin{equation}
Y = USV^T
\end{equation}
sei ihre Singulärwertzerlegung. Dann gilt:
\begin{itemize}
\item Die Spalten von $U$ sind die Eigenvektoren von $YY^T$,
\item die Spalten von $V$ sind die Eigenvektoren von $Y^TY$ und
\item die entsprechenden Singulärwerte sind die Quadratwurzeln der zugehörigen Eigenwerte.
\end{itemize}
\end{theorem}
\footnotesize
Bemerkung

* Singulärwertzerlegung und Eigenanalyse sind eng verwandt.

# Singulärwertzerlegung
\footnotesize
\underline{Beweis}
\vspace{1mm}

Wir halten zunächst fest, dass mit
\begin{equation}
\left(YY^T\right)^T = YY^T \mbox{ und } \left(Y^TY\right)^T = Y^TY,
\end{equation}
$YY^T$ und $Y^TY$ symmetrische Matrizen sind und somit Orthornomalzerlegungen haben.
Wir halten weiterhin fest, dass mit $V^TV = I_n$, $U^TU = I_m$ und $S^T = S$ gilt, dass
\begin{equation}
YY^T
= USV^T \left(USV^T\right)^T
= USV^TVS^TU^T
= USSU^T
=: U\Lambda U^T
\end{equation}
und
\begin{equation}
Y^TY
= \left(USV^T\right)^T USV^T
= VS^TU^T US^T V^T
=: V\Lambda V^T
\end{equation}
ist, wobei wir $\Lambda := SS$ definiert haben. Weil das Produkt von Diagonalmatrizen
wieder eine Diagonalmatrix ist, ist $\Lambda$ eine Diagonalmatrix und per Definition
sind $U$ und $V$ orthogonale Matrizen. Wir haben also $YY^T$ und $Y^TY$
in Form der Orthonormalzerlegungen
\begin{equation}
YY^T = U \Lambda U^T  \mbox{ und } Y^TY = V \Lambda V^T
\end{equation}
geschrieben, wobei für die Diagonalelemente von $\Lambda$ gilt, dass sie die quadrierten Werte
der Diagonalemente von $S$ sind.

$\hfill \Box$

# 
\vfill
\setstretch{3}
\Large
Eigenvektoren und Eigenwerte

Orthonormalzerlegung

Singulärwertzerlegung

**Selbstkontrollfragen**
\vfill 

# Selbstkontrollfragen
\footnotesize
\setstretch{3}
\begin{enumerate}
\item Geben Sie die Definition eines Eigenvektors und eines Eigenwertes einer quadratischen Matrix wieder.
\item Geben Sie das Theorem zur Bestimmung von Eigenwerten und Eigenvektoren wieder.
\item Geben Sie das Theorem zu den Eigenwerten und Eigenvektoren symmetrischer Matrizen wieder.
\item Geben Sie das Theorem zur orthonormalen Zerlegung einer symmetrischen Matrix wieder.
\item Geben Sie die Definition einer Singulärwertzerlegung wieder.
\item Geben Sie das Theorem zum Zusammenhang von Singulärwertzerlegung und Eigenanalyse wieder.
\end{enumerate}

# References
\footnotesize
