---
fontsize: 8pt
bibliography: 7_Referenzen.bib
citation_package: natbib
output:
  beamer_presentation:
    keep_tex: true
    includes:
      in_header: 7_header.tex
---

```{r, include = F}
source("7_R_common.R")
```

#  {.plain}
\center
```{r, echo = FALSE, out.width = "20%"}
knitr::include_graphics("7_Abbildungen/mvda_7_otto.png")
```

\vspace{2mm}

\Huge
Multivariate Datenanalyse
\vspace{6mm}

\Large
MSc Psychologie WiSe 2022/23

\vspace{6mm}
\large
Prof. Dr. Dirk Ostwald

# {.plain}
\vfill
\center
\huge
\textcolor{black}{(7) Kanonische Korrelationsanalyse}
\vfill

#
\vspace{2mm}
\textcolor{darkblue}{Modul A1/A3 Forschungsmethoden: Multivariate Verfahren | Themen}
\vspace{2mm}

\center
\footnotesize
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{lll}
Datum        & Einheit                          & Thema					                      \\\hline
14.10.2022   & Grundlagen                       & (1) Einführung 				              \\
21.10.2022   & Grundlagen                       & (2) Vektoren             	                  \\
28.10.2022   & Grundlagen                       & (3) Matrizen                                \\
04.11.2022   & Grundlagen                       & (4) Eigenanalyse                            \\
11.11.2022   & Grundlagen                       & (5) Multivariate Wahrscheinlichkeitstheorie \\
18.11.2022   & Grundlagen                       & (6) Multivariate Normalverteilungen         \\
25.11.2022   & Frequentistische Inferenz        & (7) Kanonische Korrelationsanalyse          \\
02.12.2022   & Frequentistische Inferenz        & (8) T$^2$-Tests                             \\
09.12.2022   & Frequentistische Inferenz        & (9) Einfaktorielle MANOVA                   \\
16.12.2022   & Latente Variablenmodelle         & (10) Hauptkomponentenanalyse                \\
             & \textcolor{gray}{Weihnachtspause}                                              \\
13.01.2023   & Latente Variablenmodelle         & (12) Lineare Normalverteilungsmodelle       \\
20.01.2023   & Latente Variablenmodelle         & (13) Konfirmatorische Faktorenanalyse       \\
27.01.2023   & Latente Variablenmodelle         & (14) Exploratorische Faktorenanalyse        \\ 
\end{tabular}


# Frequentistische Inferenz 
\textcolor{darkblue}{Datenanalyseszenarien}
\vspace{2mm}

\small
\renewcommand{\arraystretch}{2}
\begin{tabular}{lll}
UV
& AV
& Datenanalysemethoden
\\\hline
Univariat
& Univariat
& Korrelation, Einfache Regression,T-Tests
\\
Multivariat
& Univariat
& Multiple Korrelation, Multiple Regression, Allgemeines Lineares Modell
\\
Univariat
& Multivariat
& T$^2$-Tests, Einfaktorielle MANOVA
\\
Multivariat
& Multivariat
& Kanonische Korrelation, Multivariates Allgemeines Lineares Modell
\end{tabular}

# Frequentistische Inferenz  
\textcolor{darkblue}{Datenanalyseszenarien}
\vspace{4mm}

\small
\begin{minipage}{0.5\linewidth}
\center
\begin{tabular}{c|c}
UV         & AV          \\
$x_1$      & $y_1$       \\\hline
$x_{11}$   & $y_{11}$    \\
$x_{12}$   & $y_{12}$    \\
$x_{13}$   & $y_{13}$    \\
$\vdots$   & $\vdots$    \\
$\vdots$   & $\vdots$    \\
$\vdots$   & $\vdots$    \\
$x_{1n}$   & $y_{1n}$    \\
\end{tabular}
\vspace{2mm}

Korrelation

Einfache Regression

T-Tests
\end{minipage}
\begin{minipage}{0.5\linewidth}
\center
\begin{tabular}{ccc|c}
          & UV        &          & AV          \\
$x_1$     & $\cdots$  & $x_m$    & $y_1$       \\\hline
$x_{11}$  & $\cdots$  & $x_{m1}$ & $y_{11}$    \\
$x_{12}$  & $\cdots$  & $x_{m2}$ & $y_{12}$    \\
$x_{13}$  & $\cdots$  & $x_{m3}$ & $y_{13}$    \\
$\vdots$  & $\vdots$  & $\vdots$ & $\vdots$    \\
$\vdots$  & $\vdots$  & $\vdots$ & $\vdots$    \\
$\vdots$  & $\vdots$  & $\vdots$ & $\vdots$    \\
$x_{1n}$  & $\cdots$  & $x_{mn}$ & $y_{1n}$    \\
\end{tabular}
\vspace{2mm}

Multiple Korrelation

Multiple Regression

Allgemeines Lineares Modell

\end{minipage}


# Frequentistische Inferenz
\textcolor{darkblue}{Datenanalyseszenarien}
\vspace{4mm}

\small
\begin{minipage}{0.4\linewidth}
\center
\begin{tabular}{c|ccc}
UV        &          & AV        &             \\
$x_1$     & $y_1$    & $\cdots$  & $y_m$       \\\hline
$x_{11}$  & $y_{12}$ & $\cdots$  & $y_{m1}$    \\
$x_{12}$  & $y_{13}$ & $\cdots$  & $y_{m2}$    \\
$x_{13}$  & $y_{14}$ & $\cdots$  & $y_{m3}$    \\
$\vdots$  & $\vdots$ & $\vdots$  & $\vdots$    \\
$\vdots$  & $\vdots$ & $\vdots$  & $\vdots$    \\
$\vdots$  & $\vdots$ & $\vdots$  & $\vdots$    \\
$x_{1n}$  & $y_{1n}$ & $\cdots$  & $y_{mn}$    \\
\end{tabular}
\vspace{2mm}

T$^2$-Tests

Einfaktorielle MANOVA

\end{minipage}\hspace{2mm}
\begin{minipage}{0.6\linewidth}
\center
\begin{tabular}{ccc|ccc}
          & UV        &             &            & AV        &             \\
$x_1$     & $\cdots$  & $x_{m_x}$   & $y_1$      & $\cdots$  & $y_{m_y}$   \\\hline
$x_{11}$  & $\cdots$  & $x_{m_x1}$  & $y_{11}$   & $\cdots$  & $y_{m_y1}$  \\
$x_{12}$  & $\cdots$  & $x_{m_x2}$  & $y_{12}$   & $\cdots$  & $y_{m_y2}$  \\
$x_{13}$  & $\cdots$  & $x_{m_x3}$  & $y_{13}$   & $\cdots$  & $y_{m_y3}$  \\
$\vdots$  & $\vdots$  & $\vdots$    & $\vdots$   & $\vdots$  & $\vdots$    \\
$\vdots$  & $\vdots$  & $\vdots$    & $\vdots$   & $\vdots$  & $\vdots$    \\
$\vdots$  & $\vdots$  & $\vdots$    & $\vdots$   & $\vdots$  & $\vdots$    \\
$x_{1n}$  & $\cdots$  & $x_{m_xn}$  & $y_{1n}$   & $\cdots$  & $y_{m_yn}$  \\
\end{tabular}
\vspace{2mm}

\textbf{Kanonische Korrelationsanalyse}

Multivariates Allgemeines Lineares Modell
\end{minipage}

#
\setstretch{2.5}
\vfill
\large
Korrelation

Algebraische Grundlagen

Wahrscheinlichkeitstheoretische Grundlagen

Modellformulierung

Modellschätzung

Selbstkontrollfragen
\vfill

#
\setstretch{2.5}
\vfill
\large
**Korrelation**

Algebraische Grundlagen

Wahrscheinlichkeitstheoretische Grundlagen

Modellformulierung

Modellschätzung

Selbstkontrollfragen
\vfill


# Korrelation
\large
Anwendungsszenario
\vspace{2mm}

```{r, echo = FALSE, out.width = "80%"}
knitr::include_graphics("7_Abbildungen/mvda_7_beispielszenario_korrelation.pdf")
```

# Grundlagen
Beispieldatensatz

\center
\footnotesize
$i = 1,...,20$ Patient:innen, $y_i$ Symptomreduktion bei Patient:in $i$,  $x_i$ Anzahl Therapiestunden  von Patient:in $i$

\setstretch{1}
```{r, echo = F, warning = F}
library(MixMatrix)                                      # Normalverteilungen
set.seed(0)                                             # Ergebnisreproduzierbarkeit
n           = 20                                        # Anzahl Datenpunkte 
m_x         = 2                                         # Anzahl unabhängige Variablen
m_y         = 2                                         # Anzahl   abhängige Variablen 
x_1         = runif(n,10,30)                            # Therapiedauer
x_2         = runif(n, 0,10)                            # Therapeut:innenerfahrung 
X           = matrix(c(x_1,x_2), nrow = n)              # Designmatrix
B           = matrix(c(1,.1,                            # Wahre, unbekannte, Regressionskoeffizientenmatrix
                       1,.2), 
                     nrow  = m_y,
                     byrow = T)                         
Y           = rmatrixnorm(n=1, mean = X %*% B)          # Datenmatrix
D           = data.frame(x_1i = X[,1],
                         x_2i = X[,2],
                         y_1i = Y[,1],
                         y_2i = Y[,2])                  # Dataframe

# Datensicherung
fname       = file.path(getwd(),"7_Kanonische_Korrelationsanalyse.csv")
write.csv(D, file = fname, row.names = FALSE)
```

\setstretch{1}
```{r, echo = F}
fname       = file.path(getwd(), "7_Kanonische_Korrelationsanalyse.csv")
D           = read.table(fname, sep = ",", header = TRUE)
colnames(D) = c("y_i", "x_i")
knitr::kable(D[,c(1,3)], "pipe", col.names = c("x_i", "y_i"))
```

# Korrelation
Beispieldatensatz

```{r, echo = F, eval = F}
fname       = file.path(getwd(), "7_Kanonische_Korrelationsanalyse.csv")
D           = read.table(fname, sep = ",", header = TRUE)
graphics.off()
library(latex2exp)
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1)
plot(
D$x_1i,
D$y_1i,
pch        = 16,
xlab       = "Anzahl Therapiestunden (x)",
ylab       = "Symptomreduktion (y)",
xlim       = c(5,35),
ylim       = c(10,40))
legend(
"topleft",
TeX("$(x_i,y_i)$"),
lty         = 0,
pch         = 16,
col         = "black",
bty         = "n",
cex         = 1,
x.intersp   = 1)
dev.copy2pdf(
file        = file.path("7_Abbildungen", "mvda_7_beispieldatensatz_korrelation.pdf"),
width       = 4,
height      = 4)
```

```{r, echo = FALSE, out.width = "55%"}
knitr::include_graphics("7_Abbildungen/mvda_7_beispieldatensatz_korrelation.pdf")
```

\center
\textcolor{darkblue}{Wie stark hängen Anzahl Therapiestunden und Symptomreduktion zusammen?}

# Korrelation
\small
\begin{definition}[Korrelation]
\justifying
Die \textit{Korrelation} zweier Zufallsvariablen $\xi$ und $\ups$ ist definiert als
\begin{equation}
\rho(\xi,\ups) := \frac{\mathbb{C}(\xi,\ups)}{\sqrt{\mathbb{V}(\xi)}\sqrt{\mathbb{V}(\ups)}}
\end{equation}
wobei $\mathbb{C}(\xi,\ups)$ die Kovarianz von $\xi$ und $\ups$ und $\mathbb{V}(\xi)$ und
$\mathbb{V}(\ups)$ die Varianzen von $\xi$ und $\ups$ bezeichnen.
\end{definition}

\small
Für eine Einführung zur Korrelation siehe die entsprechenden BSc Lehreinheiten

[\textcolor{darkblue}{$\bullet$ Erwartungswert, Varianz, Kovarianz}](https://youtu.be/613-3a1Pyyg)

[\textcolor{darkblue}{$\bullet$ Korrelation}](https://youtu.be/zfI6LoX8bc4)

\footnotesize
Bemerkungen

* $\rho(\xi,\ups)$ wird auch \textit{Korrelationskoeffizient} von $\xi$ und $\ups$ genannt.
* Wir haben bereits gesehen, dass $-1 \le \rho(\xi,\ups) \le 1$ gilt.
* Wenn $\rho(\xi,\ups) = 0$ ist, werden $\xi$ und $\ups$ \textit{unkorreliert} genannt.
* Aus der Unabhängigkeit von $\xi$ und $\ups$ folgt $\rho(\xi,\ups) = 0$.
* Aus $\rho(\xi,\ups) = 0$ folgt die Unabhängigkeit von $\xi$ und $\ups$ im Allgemeinen nicht.

# Korrelation
\footnotesize
\begin{definition}[Stichprobenkorrelation]
\justifying
$(x_1,y_1),...,(x_n,y_n)$ seien Realisierungen von zweidimensionalen Zufallsvektoren
$(\xi_1,\ups_1), ..., (\xi_n,\ups_n)$. Weiterhin seien:
\begin{itemize}
\item Die Stichprobenmittel der $x_i$ und $y_i$ definiert als
\begin{equation}
\bar{x} := \frac{1}{n}\sum_{i=1}^n x_i
\mbox{ und }
\bar{y} := \frac{1}{n}\sum_{i=1}^n y_i.
\end{equation}
\item Die Stichprobenstandardabweichungen $x_i$ und $y_i$ definiert als
\begin{equation}
s_x := \sqrt{\frac{1}{n-1}(x_i - \bar{x})^2}
\mbox{ und }
s_y := \sqrt{\frac{1}{n-1}(y_i - \bar{y})^2}.
\end{equation}
\item Die Stichprobenkovarianz der $(x_1,y_1),...,(x_n,y_n)$ definiert als
\begin{equation}
c_{xy} := \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}).
\end{equation}
\end{itemize}
Dann ist die \textit{Stichprobenkorrelation} der $(x_1,y_1),...,(x_n,y_n)$ definiert als
\begin{equation}
r_{xy} := \frac{c_{xy}}{s_xs_y}
\end{equation}
und  wird auch \textit{Stichprobenkorrelationskoeffizient} genannt.
\end{definition}

# Korrelation
Beispiel
\vspace{2mm}
\tiny
\setstretch{1.2}
```{r}
# Laden des Beispieldatensatzes
fname = file.path(getwd(), "7_Kanonische_Korrelationsanalyse.csv")                # Dateipfad
D     = read.table(fname, sep = ",", header = TRUE)                              # Laden als Dataframe
x_i   = D$x_1i                                                                    # x_i Werte
y_i   = D$y_1i                                                                    # y_i Werte
n     = length(x_i)                                                              # n

# "Manuelle" Berechnung der Stichprobenkorrelation
x_bar = (1/n)*sum(x_i)                                                           # \bar{x}
y_bar = (1/n)*sum(y_i)                                                           # \bar{y}
s_x   = sqrt(1/(n-1)*sum((x_i - x_bar)^2))                                       # s_x
s_y   = sqrt(1/(n-1)*sum((y_i - y_bar)^2))                                       # s_y
c_xy  = 1/(n-1) * sum((x_i - x_bar) * (y_i - y_bar))                             # c_{xy}
r_xy  = c_xy/(s_x * s_y)                                                         # r_{xy}
print(r_xy)                                                                      # Ausgabe

# Automatische Berechnung mit cor()
r_xy  = cor(x_i,y_i)                                                             # r_{xy}
print(r_xy)                                                                      # Ausgabe
```

\center
\small
$\Rightarrow$ Anzahl Therapiestunden und Symptomreduktion sind hochkorreliert.

# Korrelation

Mechanik der Kovariationsterme

```{r, echo = FALSE, out.width = "80%"}
knitr::include_graphics("7_Abbildungen/mvda_7_korrelationsterme.pdf")
```

\center
\footnotesize
Häufige richtungsgleiche   Abweichung der $x_i$ und $y_i$ von ihren Mittelwerten $\Rightarrow$ Positive Korrelation

Häufige richtungsungleiche Abweichung der $x_i$ und $y_i$ von ihren Mittelwerten $\Rightarrow$ Negative Korrelation

Keine häufigen richtungsgleichen oder -entgegengesetzten Abweichungen $\Rightarrow$ Keine Korrelation


# Korrelation
\vspace{2mm}
Beispiele

```{r, eval = F, echo = F}
library(MASS)
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(3,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.3)


# Modellformulierung
n           = 30                                # Anzahl an Stichprobenvektoren
mu          = c(0,0)                            # Erwartungswertparameter
sigma_1     = 1                                 # \sigma_1
sigma_2     = 1                                 # \sigma_2
rho_12      = c( 0.8, 0.2, -0.4,                # \rho_12
                 0.6, 0.0, -0.6,
                 0.4,-0.2, -0.8)

# Datenrealisierung und Visualisierung
set.seed(1)
for(rho in rho_12){
  Sigma  = matrix(c(sigma_1^2             , rho*sigma_1*sigma_2,
                   rho*sigma_1*sigma_2, sigma_2^2), nrow = 2)
  xy     = mvrnorm(n, mu, Sigma)
  plot(
  xy,
  pch         = 21,
  col         = "white",
  bg          = "black",
  xlab        = TeX("$x$"),
  ylab        = TeX("$y$"),
  xlim        = c(-3,3),
  ylim        = c(-3,3),
  main        = TeX(sprintf("$r_{xy}$ = %.2f", cor(xy[,1],xy[,2]))))
}

dev.copy2pdf(
file        = file.path("7_Abbildungen", "mvda_7_korrelationsbeispiele.pdf"),
width       = 11,
height      = 11)
```

```{r, echo = FALSE, out.width = "60%"}
knitr::include_graphics("7_Abbildungen/mvda_7_korrelationsbeispiele.pdf")
```


# Korrelation  

```{r, eval = F, echo = F}
library(MASS)
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1.1,
cex.main    = 1.2,
xpd         = TRUE)

# Modellformulierung und Datenrealisierungen
set.seed(1)
n           = 40
x           = seq(-pi,pi,len = n)
eps         = rnorm(n)
y_all       = list(x + eps ,x^2 + eps, 8*cos(2*x) + eps)

# Visualisierung
for(y in y_all){
 plot(
 x,
 y,
 pch   = 21,
 bg    = "black",
 col   = "white",
 xlim  = c(-3.5,3.5),
 main  = TeX(sprintf("$r_{xy}$ = %.2f", cor(x,y))))
}

dev.copy2pdf(
file        = file.path("7_Abbildungen", "mvda_7_rlinearitaet.pdf"),
width       = 11,
height      = 4)
```
Funktionale Abhängigkeiten und Stichprobenkorrelation

\vspace{1cm}

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("7_Abbildungen/mvda_7_rlinearitaet.pdf")
```
\vspace{-5mm}

$\,$
\hspace{1cm}
$y_i = x_i + \varepsilon_i$
\hspace{1.9cm}
$y_i = x_i^2 + \varepsilon_i$
\hspace{1.2cm}
$y_i = 8 \cos(2x_i) + \varepsilon_i$

\center
$\quad\,\,\,\varepsilon_i \sim N(0,1)$


# Korrelation  
\small
\begin{theorem}[Korrelation und linear-affine Abhängigkeit]
\justifying
\normalfont
$x$ und $y$ seien zwei Zufallsvariablen mit positiver Varianz. Dann besteht genau
dann eine lineare-affine Abhängigkeit der Form
\begin{equation}
y = \beta_0 + \beta_1x \mbox{ mit } \beta_0,\beta_1\in \mathbb{R}
\end{equation}
zwischen $x$ und $y$, wenn
\begin{equation}
\rho(x,y) = 1 \mbox{ oder } \rho(x,y) = -1.
\end{equation}
\end{theorem}

\footnotesize
Bemerkungen

* Für einen Beweis und eine vertiefte Diskussion verweisen wir auf die BSc Lehreinheiten.
* Die linear-affine Abhängigkeit $y = \beta_0 + \beta_1x$  impliziert eine linear-affine Abhängigkeit $x = \tilde{\beta}_0 + \tilde{\beta}_1y$, denn
\begin{equation}
y = \beta_0 + \beta_1x
\Leftrightarrow
-\beta_0 + y = \beta_1x
\Leftrightarrow
x = -\frac{\beta_0}{\beta_1} + \frac{1}{\beta_1}y
\Leftrightarrow
x = \tilde{\beta}_0 + \tilde{\beta}_1 y
\end{equation}
mit
\begin{equation}
\tilde{\beta}_0 = -\frac{\beta_0}{\beta_1} \mbox{ und } \tilde{\beta}_1 = \frac{1}{\beta_1}.
\end{equation}


# Korrelation  
\footnotesize
\begin{theorem}[Kovarianz und Korrelation bei linear-affinen Transformationen]
\justifying
\normalfont
$\xi$ und $\upsilon$ seien Zufallsvariablen und es seien $\alpha,\beta,\gamma,\delta \in \mathbb{R}$. 
Dann gelten
\begin{equation}
\mathbb{C}(\alpha\xi + \beta, \gamma\upsilon + \delta) = \alpha\beta\mathbb{C}(\xi,\upsilon)
\end{equation}
und
\begin{equation}
\rho(\alpha\xi + \beta, \gamma\upsilon + \delta) = \rho(\xi,\upsilon).
\end{equation}
\end{theorem}

Bemerkungen

* Wir benötigen diese Aussage im Kontext der Kanonischen Korrelationsanalyse.
* Die Kovarianz zweier Zufallsvariablen ändert sich bei linear-affiner Transformation der Zufallsvariablen.
* Die Korrelation zweier Zufallsvariablen ändert sich bei linear-affiner Transformation der Zufallsvariablen nicht.

# Korrelation  
\footnotesize
\underline{Beweis}

Es gilt zunächst
\begin{align}
\begin{split}
\mathbb{C}(\alpha\xi+\beta,c\upsilon+d)
& = \mathbb{E}((\alpha\xi+\beta-\mathbb{E}(\alpha\xi+\beta))(\gamma\upsilon+\delta-\mathbb{E}(\gamma\upsilon+\delta)))    \\
& = \mathbb{E}((\alpha\xi+\beta-\alpha\mathbb{E}(\xi)-\beta)(\gamma\upsilon+\delta-\gamma\mathbb{E}(\gamma\upsilon)-\delta))   \\
& = \mathbb{E}(\alpha(\xi-\mathbb{E}(\xi))(\gamma(\upsilon -\gamma\mathbb{E}(\upsilon)))             \\
& = \mathbb{E}(\alpha\gamma((\xi-\mathbb{E}(\xi))(\upsilon -\gamma\mathbb{E}(\upsilon))))            \\
&  = \alpha\gamma\mathbb{C}(\xi,\upsilon)
\end{split}
\end{align}
Also folgt
\begin{align}
\begin{split}
\rho(\alpha\xi + \beta, \gamma\upsilon + \delta)
& = \frac{\mathbb{C}(\alpha\xi+\beta,\gamma\upsilon+\delta)}{\sqrt{\mathbb{V}(\alpha\xi+\beta)}\sqrt{\mathbb{V}(\gamma\upsilon+\delta)}} \\
& = \frac{\alpha\gamma\mathbb{C}(\xi,\upsilon)}{\sqrt{\alpha^2\mathbb{V}(\xi)}\sqrt{\gamma^2\mathbb{V}(\upsilon)}}     \\
& = \frac{\alpha\gamma\mathbb{C}(\xi,\upsilon)}{\alpha\mathbb{S}(\xi)c\mathbb{S}(\upsilon)}         \\
& = \frac{\mathbb{C}(\xi,\upsilon)}{\mathbb{S}(\xi)\mathbb{S}(\upsilon)}             \\
& = \rho(\xi,\upsilon).
\end{split}
\end{align}


# Korrelation
Anwendungsszenario Kanonische Korrelation
\vspace{3mm}
```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("7_Abbildungen/mvda_7_beispielszenario_kanonische_korrelation.pdf")
```

# Korrelation
Beispieldatensatz Kanonische Korrelation

\footnotesize
$i = 1,...,n$ Patient:innen

\center
$y_{1i}$ BDI Score Reduktion, $y_{2i}$ Glucocorticoid Reduktion, $x_{1i}$ Therapiedauer, $x_{2i}$ Erfahrung Psychotherapeut:in, 

\setstretch{1}
```{r, echo = F}
fname       = file.path(getwd(), "7_Kanonische_Korrelationsanalyse.csv")
D           = read.table(fname, sep = ",", header = TRUE)
knitr::kable(D, "pipe")
```


# Korrelation
\textcolor{darkblue}{Grundzüge der Kanonischen Korrelationsanalyse}
\footnotesize

Die Datenvektoren $x_{1i},...,x_{m_xi}, i = 1,...,n$ werden als u.i.v. Realisierungen eines Zufallsvektors $x$ interpretiert.
Die Datenvektoren $y_{1i},...,y_{m_yi}, i = 1,...,n$ werden als u.i.v. Realisierungen eines Zufallsvektors $y$ interpretiert.

Die "erste kanonische Korrelation" ist die maximale Korrelation von Linearkombinationen
von $x$ und $y$; wir bezeichnen die Linearkombinationen von $x$ und $y$ mit
Vektoren $a \in \mathbb{R}^{m_x}$ und $b \in \mathbb{R}^{m_y}$ mit
\begin{equation}
\xi      = a^Tx = a_1x_1 + \cdots + a_{m_x}x_{m_x}
\mbox{ und }
\upsilon = b^Ty = b_1y_1 + \cdots + b_{m_y}y_{m_y}
\end{equation}
$\xi$ und $\upsilon$ sind dann als Linearkombinationen von Zufallsvariablen selbst Zufallsvariablen;
Die Korrelation von $\xi$ und $\upsilon$ bezeichnen wir mit $\rho(\xi,\upsilon)$

Wenn die Zufallsvektoren $x$ als unabhängige Variable und $y$ als abhängige Variable
interpretiert werden, dann kann $\xi= a^Tx$ als "bester Prädiktor" und
$\upsilon  = b^Ty$ als "am besten prädizierbares Kriterium" interpretiert werden.
Kanonische Korrelationsanalyse fragt damit nach Parametern $a \in \mathbb{R}^{m_x}$
und $b\in \mathbb{R}^{m_y}$ für die $\rho(\xi,\upsilon)$ maximal ist.

Für Skalare $\alpha,\beta\in \mathbb{R}$ sind die Korrelationen $\rho(a^Tx,b^Ty)$ und
$\rho((\alpha a^T)x,(\beta b^T)y)$ allerdings
identisch (siehe unten). Man sucht deshalb Parameter
$a \in \mathbb{R}^{m_x}$ und $b\in \mathbb{R}^{m_y}$  für die $\rho(\xi,\upsilon)$ maximal ist und für die
$a^Tx$ und $b^Ty$ jeweils eine Varianz von 1 haben, also $\mathbb{V}(\xi) = \mathbb{V}(\upsilon) = 1$ gilt.

Anders ausgedrückt: Die Varianzen von $a^Tx$ und $b^Ty$ und die Varianzen von Linearkombinationen
von $x$ und $y$ mit beliebigen skalaren Vielfachen von $a$ und $b$ sind im Sinne
der ersten Aussage des Theorems zur Kovarianz und Korrelation bei linear-affinen 
Transformationen verschieden.

# Korrelation
\textcolor{darkblue}{Grundzüge der Kanonischen Korrelationsanalyse}

\footnotesize
Insbesondere gilt einen $m_x$-dimensionalen Zufallsvektoren $x$ und einen
$m_y$-dimensionalen Zufallsvektor $y$, $a \in \mathbb{R}^{m_x},b \in \mathbb{R}^{m_y}$,
$\alpha,\beta \in \mathbb{R}$ und die Linearkombinationen
$\xi := a^Tx$ und $\upsilon := b^Ty$ basierend auf dem Theorems zur Kovarianz 
und Korrelation bei linear-affinen allerdings auch, dass
\begin{equation*}
\rho(\xi,\upsilon) = \rho(\alpha\xi,\beta\upsilon)   
\Leftrightarrow
\rho(a^Tx, b^Ty)   = \rho(\alpha(a^Tx),\beta(b^Ty))  
\Leftrightarrow
\rho(a^Tx, b^Ty)   = \rho((\alpha a^T)x),(\beta b^T)y).
\end{equation*}
Die Korrelation von $a^Tx$ und $b^Ty$ und die Korrelationen von Linearkombinationen
von $x$ und $y$ mit beliebigen skalaren Vielfachen von $a$ und $b$ sind also gleich.

Zur Entwicklung der Kanonischen Korrelationsanalyse folgen wir @mardia_1979, Kapitel 10.
Dabei werden die Zufallsvektoren $x$ und $y$ in einem Zufallsvektor 
\begin{equation}
z := \begin{pmatrix} x \\ y \end{pmatrix}
\end{equation}
zusammengefasst, für den wir durchgängig annehmen, dass $\mathbb{E}(z) = 0_{m}$ 
mit $m = m_x + m_y$. Dies entspricht auf der Anwendungsebene der Subtraktion 
des Stichprobenmittels von den beobachteten Daten vor Durchführung der Kanonischen
Korrelationsanalyse

Der mathematische Fokus der Entwicklung nach @mardia_1979, Kapitel 10 ist auf der 
Kovarianzmatrix $\mathbb{C}(z)$. Speziell ergeben sich die Kovarianzen von 
Linearkombinationen von $x$ und $y$ aus Matrixprodukten von $\mathbb{C}(z)$ und 
es können einige Matrixtheoreme, dieim Folgenden diskutiert werden, auf diese 
Matrixprodukte angewendet werden. Generell wird in der Entwicklung nach @mardia_1979, 
Kapitel 10 ein restringierter Optimierungsansatz mithilfe der Lagrangefunktion 
zugunsten der Eigenanalyse von Matrixprodukten supprimiert. Für die Entwicklung
mit einem Lagrangeansatz, siehe zum Beispiel @anderson_2003, Kapitel 12.

#
\setstretch{2.5}
\vfill
\large
Korrelation

**Algebraische Grundlagen**

Wahrscheinlichkeitstheoretische Grundlagen

Modellformulierung

Modellschätzung

Selbstkontrollfragen
\vfill


# Algebraische Grundlagen
\footnotesize
\begin{definition}[Symmetrische Quadratwurzel einer Matrix]
\justifying
$A \in \mathbb{R}^{m \times m}$ sei eine invertierbare symmetrische Matrix mit
positiven Eigenwerten. Dann sind für $r \in \mathbb{N}^0$ und $s \in \mathbb{N}$
die rationalen Potenzen von $A$ einer orthonormalen Matrix $Q \in \mathbb{R}^{m \times m}$
der Eigenvektoren von $A$ und einer Diagonalmatrix  $\Lambda = \mbox{diag}(\lambda_i) \in \mathbb{R}^{m \times m}$
der zugehörigen Eigenwerte $\lambda_1,...,\lambda_m$ von $A$ definiert als
\begin{equation}
A^{r/s} = Q \Lambda^{r/s} Q^T \mbox{ mit } \Lambda^{r/s} = \mbox{diag}\left(\lambda_i^{r/s}\right).
\end{equation}
Der Spezialfall $r:= 1, s := 2$ wird als symmetrische Quadratwurzel von $A$ bezeichnet
und hat die Form
\begin{equation}
A^{1/2} = Q\Lambda^{1/2}Q^T \mbox{ mit } \Lambda^{1/2} = \mbox{diag}\left(\lambda_i^{1/2}\right).
\end{equation}
\end{definition}
\setstretch{1}
Bemerkungen

* Offenbar gilt
\begin{equation}
\left(A^{1/2} \right)^2 = Q\Lambda^{1/2}Q^TQ\Lambda^{1/2}Q^T = Q\Lambda^{1/2}\Lambda^{1/2}Q^T =  Q\Lambda Q^T = A.
\end{equation}
* Weiterhin gilt
\begin{equation}
\left(A^{-1/2} \right)^2 = Q\Lambda^{-1/2}Q^TQ\Lambda^{-1/2}Q^T = Q\Lambda^{-1}Q^T = A^{-1}.
\end{equation}
Die vorletzte Gleichung mag überraschen, aber es gilt ja zum Beispiel
\begin{equation}
4^{-1/2}\cdot 4^{-1/2} = \frac{1}{\sqrt{4}}\cdot\frac{1}{\sqrt{4}} = \frac{1}{4} = 4^{-1}.
\end{equation}

# Algebraische Grundlagen
\footnotesize
\begin{theorem}[Eigenwerte und Eigenvektoren von Matrixprodukten]
\justifying
\normalfont
Für $A \in \mathbb{R}^{n \times m}$ und $B \in \mathbb{R}^{m \times n}$ sind
die Eigenwerte von $AB \in \mathbb{R}^{n \times n}$ und $BA \in \mathbb{R}^{m \times m}$
gleich. Weiterhin gilt, dass für einen Eigenvektor $v$ zu einem von Null
verschiedenen Eigenwert $\lambda$ von $AB$ $w := Bv$ ein Eigenvektor von $BA$ ist.
\end{theorem}

Bemerkungen

* Für einen Beweis siehe @mardia_1979, S. 468.

\vspace{1mm}
\tiny
\setstretch{1.2}
```{r}
A   = matrix(1:6, nrow = 2,  byrow = T)           # Matrix A \in \mathbb{R}^{2 x 3}
B   = matrix(1:6, ncol = 2,  byrow = T)           # Matrix B \in \mathbb{R}^{3 x 2}
EAB = eigen(A %*% B)                              # Eigenanalyse von AB \in \mathbb{R}^{2 \times 2}
EBA = eigen(B %*% A)                              # Eigenanalyse von BA \in \mathbb{R}^{3 \times 3}
w   = B %*% EAB$vectors[,1]                       # Eigenvektor von BA
cat("Eigenwerte von AB :"  , EAB$values[1:2],
    "\nEigenwerte von BA :", EBA$values[1:2],
    "\nBAw mit w = Bv    :", B %*% A %*% w,
    "\nlw mit w = Bv     :", EBA$values[1] * w)

```

# Algebraische Grundlagen
\footnotesize
\begin{theorem}[Eigenwert und Eigenvektor eines Matrixvektorprodukts]
\justifying
\normalfont
Für $A \in \mathbb{R}^{n \times m}, B \in \mathbb{R}^{p \times n}, a \in \mathbb{R}^m$
und $b \in \mathbb{R}^p$ gilt, dass der einzige von Null verschiedene Eigenwert von
$Aab^TB \in \mathbb{R}^{n \times n}$ gleich $b^T BAa$ mit zugehörigem Eigenvektor $Aa$ ist.
\end{theorem}

Bemerkungen

* Für einen Beweis siehe @mardia_1979, S. 468.

\vspace{1mm}
\tiny
\setstretch{1.2}
\vspace{1mm}
\tiny
\setstretch{1.2}
```{r}
A      = matrix(1:6, nrow = 2,  byrow = T)     # Matrix A \in \mathbb{R}^{2 x 3}
B      = matrix(1:8, ncol = 2,  byrow = T)     # Matrix B \in \mathbb{R}^{4 x 2}
a      = matrix(1:3, nrow = 3,  byrow = T)     # Vektor a \in \mathbb{R}^{3 x 1}
b      = matrix(1:4, nrow = 4,  byrow = T)     # Vektor b \in \mathbb{R}^{4 x 1}
EAabTB = eigen(A %*% a %*% t(b) %*% B)         # Eigenanalyse von Aab^TB \in \mathbb{R}^{4 x 4}
cat("Eigenwerte von AabTB :", EAabTB$values,
    "\nbTBAa                :", t(b) %*% B %*% A %*% a,
    "\nAa                   :", A %*% a,
    "\n(AabTB)Aa            :",(A %*% a %*% t(b) %*% B) %*% A %*% a,            # Mv
    "\n(bTBAa)Aa            :",as.vector((t(b) %*% B %*% A %*% a)) * (A %*% a)) # = \lambda v
```

# Algebraische Grundlagen
\footnotesize
\begin{theorem}[Maximierung quadratischer Formen mit Nebenbedingungen]
\justifying
\normalfont
$A \in \mathbb{R}^{m \times m}, B \in \mathbb{R}^{m \times m} \mbox{ p.d.}$
seien symmetrische Matrizen und $\lambda_1$ sei der größte Eigenwert von $B^{-1}A$
mit assoziertem Eigenvektor $v_1 \in \mathbb{R}^m$. Dann ist $\lambda_1$ eine Lösung des
Optimierungsproblems
\begin{equation}\label{eq:opt_1}
\max_{x} x^TAx \mbox{ unter der Nebenbedingung } x^TBx = 1.
\end{equation}
\end{theorem}

Bemerkungen

* Das Theorem ist direkt durch die kanonische Korrelationsanalyse motiviert.
* $\max_{x} f(x)$ ist das Maximum einer Funktion $f$, also der Wert der Funktion an der Maximumstelle $x$
* $\argmax_{x} f(x)$ ist die Maximumstelle einer Funktion, also ein Wert in der Definitionsmenge von $f$.
* Nach Wortlaut des Theorems gilt also für die Funktion
\begin{equation}
f : \mathbb{R}^m \to \mathbb{R}, x \mapsto f(x) := x^TAx,
\end{equation}
dass
\begin{equation}
v_1 = \argmax_{x} x^TAx \mbox{ unter der Nebenbedingung } x^TBx = 1
\end{equation}
und dass
\begin{equation}
\lambda_1 = \max_{x} x^TAx \mbox{ unter der Nebenbedingung } x^TBx = 1.
\end{equation}


# Algebraische Grundlagen
\footnotesize
\underline{Beweis}

$B^{1/2}$ sei die symmetrische Quadratwurzel von $B$ und es sei
\begin{equation}
y := B^{1/2}x \Leftrightarrow x = B^{-1/2}y
\end{equation}
Dann kann mit der symmetrischen Matrix
\begin{equation}
K := B^{-1/2}AB^{-1/2} \in \mathbb{R}^{m \times m}
\end{equation}
das Optimierungsproblem \eqref{eq:opt_1} geschrieben werden als
\begin{equation}\label{eq:opt_2}
\max_{y} y^T K y
\mbox{ unter der Nebenbedingung }
y^Ty = 1.
\end{equation}
Dies gilt, weil
\begin{equation}
\max_{x} x^TAx
\Leftrightarrow
\max_{y} \left(B^{-1/2}y\right)^TA\left(B^{-1/2}y\right)
\Leftrightarrow
\max_{y} y^TB^{-1/2}AB^{-1/2}y
\Leftrightarrow
\max_{y} y^TKy
\end{equation}
und
\begin{equation}
x^TBx = 1
\Leftrightarrow y^T B^{-1/2}BB^{-1/2}y = 1
\Leftrightarrow y^Ty = 1.
\end{equation}
Weil $K$ eine symmetrische Matrix ist, existiert die Orthonormalzerlegung (vgl. (2) Matrizen)
\begin{equation}
K = Q\Lambda Q^T,
\end{equation}
wobei die Spalten der orthogonalen Matrix $Q$ die Eigenvektoren von $K$ und die
Diagonalemente von $\Lambda$ die zugehörigen Eigenwerte von $K$ sind.

# Algebraische Grundlagen
\footnotesize
\underline{Beweis (fortgeführt)}

Mit der orthogonalen Matrix $Q$ aus obiger Orthornomalzerlegung sei nun
\begin{equation}
z := Q^Ty \Leftrightarrow y := Qz.
\end{equation}
Dann kann das Optimierungsproblem \eqref{eq:opt_2} geschrieben werden als
\begin{equation}\label{eq:opt_3}
\max_{z} \sum_{i = 1}^m \lambda_i z_i^2 \mbox{ unter der Nebenbedingung } z^Tz = 1,
\end{equation}
weil
\begin{equation}
\max_{y} y^TKy
\Leftrightarrow
\max_{z} (Qz)^TK(Qz)
\Leftrightarrow
\max_{z} z^TQ^TQ\Lambda Q^TQz
\Leftrightarrow
\max_{z} z^T\Lambda z
\Leftrightarrow
\max_{z} \sum_{i=1}^m \lambda_i z_i^2
\end{equation}
und
\begin{equation}
y^Ty = 1
\Leftrightarrow
(Qz)^T Qz = 1
\Leftrightarrow
z^T Q^TQz = 1
\Leftrightarrow
z^T z = 1.
\end{equation}


# Algebraische Grundlagen
\footnotesize
\underline{Beweis (fortgeführt)}

Die Eigenwerte von $K$ seien nun absteigend sortiert, also $\lambda_1 \ge \cdots \ge \lambda_m$.
Dann gilt für das Optimierungsproblem \eqref{eq:opt_3}, dass
\begin{equation}
\max_{z} \sum_{i = 1}^m \lambda_i z_i^2 \le \lambda_1,
\end{equation}
weil
\begin{equation}
\max_{z} \sum_{i = 1}^m \lambda_i z_i^2
\le
\max_{z} \sum_{i = 1}^m \lambda_1 z_i^2
=
\lambda_1 \max_{z} \sum_{i = 1}^m z_i^2
=
\lambda_1
\end{equation}
wobei sich die letzte Gleichung aus der Nebenbedingung $z^Tz=1$ ergibt. Schließlich
gilt
\begin{equation}
\max_{z} \sum_{i = 1}^m \lambda_i z_i^2  = \lambda_1,
\end{equation}
für $z := e_1 = (1,0,...,0)^T$. Zusammenfassend heißt das, dass $z = e_1$ eine Lösung
des Optimierungsproblem \eqref{eq:opt_3} ist und das $\lambda_1$ das entsprechende
Maximum ist.

# Vorbemerkungen
\footnotesize
\underline{Beweis (fortgeführt)}

Damit ergibt sich aber sofort, dass dann
\begin{equation}
y = Qz = Qe_1 =  q_1 \mbox{ und } x = B^{-1/2}q_1
\end{equation}
Lösungen der äquivalenten Optimierungsprobleme \eqref{eq:opt_2} und \eqref{eq:opt_1},
respektive, sind. Nach Konstruktion ist $q_1$ ein Eigenvektor von $B^{-1/2}AB^{-1/2}$ und nach
obigem Theorem zu Eigenwerten und Eigenvektoren von Matrixprodukten damit auch
ein Eigenvektor von
\begin{equation}
B^{-1/2}B^{-1/2}A = B^{-1}A
\end{equation}
und die zugehörigen Eigenwerte sind gleich. Damit aber folgt, dass der
größte Eigenwert von $B^{-1}A$ und sein assoziierter Eigenvektor eine Lösung von
\begin{equation}
\max_{x} x^TAx \mbox{ unter der Nebenbedingung } x^TBx = 1.
\end{equation}
ist.
$\hfill\Box$

#
\setstretch{2.5}
\vfill
\large
Korrelation

Algebraische Grundlagen

**Wahrscheinlichkeitstheoretische Grundlagen**

Modellformulierung

Modellschätzung

Selbstkontrollfragen
\vfill

# Wahrscheinlichkeitstheoretische Grundlagen
\footnotesize
\begin{theorem}[Kovarianzmatrizen von Zufallsvektoren]
\normalfont
\justifying
Es seien
\begin{equation}
z = \begin{pmatrix} x \\ y\end{pmatrix}
\mbox{ mit }
\mathbb{E}(z)  := 0_m
\end{equation}
ein $m_x + m_y$-dimensionaler Zufallsvektor und sein Erwartungswertvektor,
respektive. Dann kann die $m \times m$ Kovarianzmatrix  $z$ geschrieben werden als
\begin{equation}
\mathbb{C}(z) =
\begin{pmatrix}
\Sigma_{xx} & \Sigma_{xy} \\
\Sigma_{yx} & \Sigma_{yy} \\
\end{pmatrix}
\in \mathbb{R}^{m \times m}
\end{equation}
wobei
\begin{align}
\begin{split}
\Sigma_{xx} & := \mathbb{E}\left(xx^T\right) \in \mathbb{R}^{m_x \times m_x}\\
\Sigma_{xy} & := \mathbb{E}\left(xy^T\right) \in \mathbb{R}^{m_x \times m_y}\\
\Sigma_{yx} & := \mathbb{E}\left(yx^T\right) \in \mathbb{R}^{m_y \times m_x}\\
\Sigma_{yy} & := \mathbb{E}\left(yy^T\right) \in \mathbb{R}^{m_x \times m_y}
\end{split}
\end{align}
\end{theorem}

# Wahrscheinlichkeitstheoretische Grundlagen
\footnotesize
\underline{Beweis}

\renewcommand{\arraystretch}{1.2}
Nach Definition der Kovarianzmatrix eines Zufallsvektors gilt
\begin{align}
\begin{split}
\mathbb{C}(z)
& = \mathbb{E}\left((z - \mathbb{E}(z))(z - \mathbb{E}(z))^T \right) \\
& = \mathbb{E}\left((z - 0_m)(z - 0_m)^T \right) \\
& = \mathbb{E}\left(zz^T\right)\\
& = \mathbb{E}\left(\begin{pmatrix} x \\ y \end{pmatrix} \begin{pmatrix} x^T & y^T \end{pmatrix} \right) \\
& = \mathbb{E}\left(\begin{pmatrix} xx^T & xy^T \\ yx^T & yy^T \end{pmatrix}\right)
\\
& =
\begin{pmatrix}
\mathbb{E}\left(xx^T\right) & \mathbb{E}\left(xy^T\right) \\
\mathbb{E}\left(yx^T\right) & \mathbb{E}\left(yy^T\right)
\end{pmatrix}
\\
& =
\begin{pmatrix}
\Sigma_{xx} & \Sigma_{xy} \\
\Sigma_{yx} & \Sigma_{yy} \\
\end{pmatrix}
\end{split}
\end{align}
$\hfill\Box$

# Modellformulierung
\footnotesize
\begin{theorem}[Linearkombinationen von Zufallsvektorpartitionen]
\justifying
\normalfont
Es sei
\begin{equation}
z = \begin{pmatrix} x \\ y \end{pmatrix}
\mbox{ mit }
\mathbb{E}(x) = 0_m
\mbox{ und }
\mathbb{C}(z) =
\begin{pmatrix}
\Sigma_{xx} & \Sigma_{xy} \\
\Sigma_{yx} & \Sigma_{yy} \\
\end{pmatrix}
\end{equation}
ein $m$-dimensionaler partitionierter Zufallsvektor sowie sein
Erwartungswertvektor und seine Kovarianzmatrix, respektive. Weiterhin seien für
$a \in \mathbb{R}^{m_x}$ und $b\in\mathbb{R}^{m_y}$
die Zufallsvariablen
\begin{equation}
\xi := a^T x \mbox{ und } \upsilon := b^T y
\end{equation}
als Linearkombinationen der Komponenten von $x$ und $y$ definiert.
Dann gelten
\begin{itemize}
\item[(1)] $\mathbb{V}(\xi) = a^T\Sigma_{xx}a$
\item[(2)] $\mathbb{V}(\upsilon) = b^T\Sigma_{yy}b$
\item[(2)] $\rho(\xi,\upsilon) = a^T \Sigma_{xy}b$, wenn  $\mathbb{V}(\xi) = 1$ und $\mathbb{V}(\upsilon) = 1$.
\end{itemize}
\end{theorem}

Bemerkungen

* Die Varianz der Zufallsvariable $a^Tx$ ergibt sich als "doppelte Linearkombination" von $\Sigma_{xx}$.
* Die Varianz der Zufallsvariable $b^Ty$ ergibt sich als "doppelte Linearkombination" von $\Sigma_{yy}$.
* Die Korrelation der Zufallsvariablen  $a^Tx$ und $b^Ty$ ergibt sich  "doppellte Linearkombination" von $\Sigma_{xy}$.

# Modellformulierung
\footnotesize
\underline{Beweis von (1) und (2)}

Wir betrachten zunächst die Varianz von $\xi$. Mit dem Varianzverschiebungssatz 
(vgl. [\textcolor{darkblue}{Erwartungswert, Varianz, Kovarianz}](https://youtu.be/613-3a1Pyyg)) 
gilt
\begin{align}
\begin{split}
\mathbb{V}(\xi)
& = \mathbb{E}\left(\xi \xi \right) - \mathbb{E}(\xi)\mathbb{E}(\xi) \\
& = \mathbb{E}\left((a^Tx) (a^Tx)\right) - \mathbb{E}\left(a^Tx\right)\mathbb{E}\left(a^Tx\right) \\
& = \mathbb{E}\left((a^Tx) (a^Tx)^T\right) - \mathbb{E}\left(a^Tx\right)\mathbb{E}\left(a^Tx\right) \\
& = \mathbb{E}\left(a^Txx^Ta\right) - \mathbb{E}\left(a^Tx\right)\mathbb{E}\left(a^Tx\right) \\
& = a^T\mathbb{E}\left(xx^T\right)a - a^T\mathbb{E}(x)a^T\mathbb{E}(x) \\
& = a^T\mathbb{E}\left(xx^T\right)a - a^T0_{m_x}a^T0_{m_x} \\
& = a^T\Sigma_{xx}a. \\
\end{split}
\end{align}
Der Beweis zur Varianz von $\upsilon$ folgt dann analog.

# Modellformulierung
\footnotesize
\underline{Beweis von (3)}

Mit der Definition der Korrelation von Zufallsvariablen und mit $\mathbb{V}(\xi) = \mathbb{V}(\upsilon) = 1$
und dem Kovarianzverschiebungssatz (vgl. [\textcolor{darkblue}{Erwartungswert, Varianz, Kovarianz}](https://youtu.be/613-3a1Pyyg)) 
gilt
\begin{align}
\begin{split}
\rho(\xi,\upsilon)
& = \frac{\mathbb{C}(\xi,\upsilon)}{\sqrt{\mathbb{V}(\xi)}\sqrt{\mathbb{V}(\upsilon)}} \\
& = \frac{\mathbb{C}(\xi,\upsilon)}{\sqrt{1}\sqrt{1}} \\
& = \mathbb{C}(\xi,\upsilon) \\
& = \mathbb{E}(\xi\upsilon) - \mathbb{E}(\xi)\mathbb{E}(\upsilon) \\
& = \mathbb{E}\left((a^Tx)(b^Ty)\right) - \mathbb{E}(a^Tx)\mathbb{E}(b^Ty) \\
& = \mathbb{E}\left((a^Tx)(b^Ty)^T\right) - \mathbb{E}(a^Tx)\mathbb{E}(b^Ty) \\
& = \mathbb{E}\left(a^T xy^Tb \right) - \mathbb{E}(a^Tx)\mathbb{E}(b^Ty) \\
& = a^T\mathbb{E}\left(xy^T \right)b - a^T\mathbb{E}(x)b^T\mathbb{E}(y) \\
& = a^T\mathbb{E}\left(xy^T \right)b - a^T0_{m_x}b^T0_{m_y} \\
& = a^T\Sigma_{xy}b. \\
\end{split}
\end{align}
$\hfill\Box$

#
\setstretch{2.5}
\vfill
\large
Korrelation

Algebraische Grundlagen

Wahrscheinlichkeitstheoretische Grundlagen

**Modellformulierung**

Modellschätzung

Selbstkontrollfragen
\vfill


# Modellformulierung
\setstretch{1.2}
\vspace{1mm}
\footnotesize
\begin{definition}[Kanonische Koeffizientenvektoren, Variate, Korrelationen]
\justifying
Es seien
\begin{equation}
z = \begin{pmatrix} x \\ y \end{pmatrix}
\mbox{ mit }
\mathbb{E}(z) := 0_m
\mbox{ und }
\mathbb{C}(z) :=
\begin{pmatrix}
\Sigma_{xx} & \Sigma_{xy} \\
\Sigma_{yx} & \Sigma_{yy} \\
\end{pmatrix}
\in \mathbb{R}^{m \times m}
\end{equation}
ein $m$-dimensionaler partitionierter Zufallsvektor sowie sein Erwartungswert
und seine Kovarianzmatrix, respektive. Weiterhin sei
\begin{equation}
K := \Sigma_{xx}^{-1/2}\Sigma_{xy}\Sigma_{yy}^{-1/2} \in \mathbb{R}^{m_x \times m_y}
\end{equation}
mit der Singulärwertzerlegung
\begin{equation}
K = A \Lambda B^T,
\end{equation}
wobei
\begin{equation}
A       := \begin{pmatrix} \alpha_1 & \cdots & \alpha_k \end{pmatrix} \in \mathbb{R}^{m_x \times m_y}
\mbox{ und }
B       := \begin{pmatrix} \beta_1  & \cdots &  \beta_k \end{pmatrix} \in \mathbb{R}^{m_y \times m_y}
\end{equation}
die orthogonalen Matrix der Eigenvektoren von $KK^T$  und die orthogonale Matrix
der Eigenvektoren von $K^TK$, respektive, bezeichnen und
\begin{equation}
\Lambda := \mbox{diag}\left(\lambda^{1/2}_1,...,\lambda_k^{1/2}\right) \in \mathbb{R}^{m_y \times m_y},
\end{equation}
die Diagonalmatrix der Quadratwurzeln der zugehörigen Eigenvektoren bezeichnet.
Schließlich seien für $i = 1,...,k$
\begin{equation}
a_i := \Sigma_{xx}^{-1/2}\alpha_i  \in \mathbb{R}^{m_x} \mbox{ und } b_i := \Sigma_{yy}^{-1/2}\beta_i \in \mathbb{R}^{m_y}.
\end{equation}
Dann heißen für $i = 1,...,k$
\begin{itemize}
\itemsep0mm
\item[(1)] $a_i \in \mathbb{R}^{m_x}$ und $b_i \in \mathbb{R}^{m_y}$ die \textit{$i$ten kanonischen Koeffizientenvektoren},
\item[(2)] die Zufallsvektoren $\xi_i := a_i^Tx$ und $\upsilon_i := b_i^Ty$ die $i$ten \textit{$i$ten kanonischen Variaten} und
\item[(3)] $\rho_i := \lambda_i^{1/2}$ die \textit{$i$te kanonische Korrelation}.
\end{itemize}
\end{definition}

# Modellformulierung
\footnotesize
\begin{theorem}[Eigenschaften kanonischer Korrelationen und Variaten]
\normalfont
\justifying
Es seien
\begin{equation}
z = \begin{pmatrix} x \\ y \end{pmatrix}
\mbox{ mit }
\mathbb{E}(z) := 0_m
\mbox{ und }
\mathbb{C}(z) :=
\begin{pmatrix}
\Sigma_{xx} & \Sigma_{xy} \\
\Sigma_{yx} & \Sigma_{yy} \\
\end{pmatrix}
\in \mathbb{R}^{m \times m}
\end{equation}
ein $m$-dimensionaler partitionierter Zufallsvektor sowie sein Erwartungswert
und seine Kovarianzmatrix, respektive. Weiterhin seien für $i = 1,...,k$ die kanonischen Koeffizientenvektoren $a_i, b_i$,
die kanonischen Variaten $\xi,\upsilon_i$ und die kanonischen Korrelationen $\rho_i$
definiert wie oben. Dann gilt, dass für $1 \le r \le k$ das Maximum des $r$ten
restringierten Optimierungsproblems
\begin{equation}
\phi_r = \max_{a,b} a^T\Sigma_{xy}b
\end{equation}
unter den Nebenbedingungen
\begin{equation}
a^T\Sigma_{xx}a   = 1,
\quad
b^T\Sigma_{yy}b   = 1,
\quad
a_i^T\Sigma_{xx}a = 0 \mbox{ für } i = 1,...,r-1
\end{equation}
(1) den Wert $\phi_r = \rho_r$ hat und (2) bei $a = a_r$ und $b = b_r$ angenommen wird.
\end{theorem}
\vspace{-1mm}
\setstretch{1.2}
Bemerkungen
\vspace{-2mm}
\begin{itemize}
\item $\phi_1$ ist die größtmögliche Korrelation von $\xi = a^Tx$ und $\upsilon = b^Ty$ unter den Nebenbedingungen
\begin{itemize}
\footnotesize
\item[$\circ$] $\mathbb{V}(\xi) =  a^T\Sigma_{xx}a = 1$ und $\mathbb{V}(\upsilon) = b^T\Sigma_{yy}b = 1$
\end{itemize}
\item $\phi_r$ ist die größtmögliche Korrelation von $\xi = a^Tx$ und $\upsilon = b^Ty$  unter den Nebenbedingungen
\begin{itemize}
\footnotesize
\item[$\circ$] $\mathbb{V}(\xi) = a^T\Sigma_{xx}a = 1$ und $\mathbb{V}(\upsilon) = b^T\Sigma_{yy}b = 1$
\item[$\circ$]  $\mathbb{C}(\xi_i,\xi) = a_i^T\Sigma_{xx}a = 0$  für die ersten $i = 1,...,r-1$ kanonischen Variaten $\xi_i$
\end{itemize}
\end{itemize}


# Modellformulierung
\footnotesize
\underline{Beweis}

Wir betrachten das restringierte Optimierungsproblem
\tiny
\begin{equation}
\phi_r^2 = \max_{a,b} \left(a^T\Sigma_{xy}b\right)^2\,
\mbox{ u.d.N. }
a^T\Sigma_{xx}a   = 1,\,
b^T\Sigma_{yy}b   = 1,\,
a_i^T\Sigma_{xx}a = 0, i = 1,...,r-1
\end{equation}
\footnotesize
Wir folgen @mardia_1979, S. 284 und gehen schrittweise vor, d.h. wir lösen
das restringierte Optimierungsproblem
\tiny
\begin{equation}
\phi_r^2 = \max_{a} \left(\max_{b} \left(a^T\Sigma_{xy}b\right)^2 \mbox{ u.d.N.} b^T\Sigma_{yy}b   = 1\right)
\mbox{ u.d.N. } a^T\Sigma_{xx}a   = 1,\, a_i^T\Sigma_{xy}a = 0,  i = 1,...,r-1
\end{equation}
\footnotesize
von innen nach außen.

Schritt (1)

Wir wählen wir zunächst ein festes $a \in \mathbb{R}^m$ und betrachten das
restringierte Optimierungsproblem
\begin{equation}
\max_{b} \left(a^T\Sigma_{xy}b\right)^2
\mbox{ u.d.N. }
b^T\Sigma_{yy}b   = 1
\end{equation}
Dieses Optimierungsproblem kann geschrieben werden als
\begin{equation}\label{eq:kka_opt_1}
\max_{b} b^T\Sigma_{yx}aa^T\Sigma_{xy}b
\mbox{ u.d.N. }
b^T\Sigma_{yy}b = 1,
\end{equation}
weil gilt, dass
\begin{equation}
\left(a^T\Sigma_{xy}b\right)^2
= \left(a^T\Sigma_{xy}b\right)\left(a^T\Sigma_{xy}b\right)
= \left(a^T\Sigma_{xy}b\right)^T a^T\Sigma_{xy}b
= b^T\Sigma_{yx}aa^T\Sigma_{xy}b.
\end{equation}


# Modellformulierung
\footnotesize
\underline{Beweis (fortgeführt)}

Das Optimierungsproblem \eqref{eq:kka_opt_1} kann nun mithilfe des Theorems
zur Maximierung quadratischer Formen mit Nebenbedingen gelöst werden. Im Sinne
dieses Theorems setzen wir dazu
\begin{equation}
A := \Sigma_{yx}aa^T\Sigma_{xy} \mbox{ und } B := \Sigma_{yy}.
\end{equation}
Dann hat \eqref{eq:kka_opt_1} die Form
\begin{equation}\label{eq:kka_opt_2}
\max_{b} b^TAb
\mbox{ unter der Nebenbedingung }
b^TBb = 1,
\end{equation}
Das Maximum von \eqref{eq:kka_opt_2} entspricht nach dem Theorem  zur Maximierung
quadratischer Formen mit Nebenbedingungen dem größten Eigenwert von
\begin{equation}
B^{-1}A = \Sigma_{yy}^{-1}\Sigma_{yx}aa^T\Sigma_{xy}
\end{equation}
Der größte Eigenwert von $\Sigma_{yy}^{-1}\Sigma_{yx}aa^T\Sigma_{xy}$ wiederum
kann mithilfe des Theorems zum Eigenwert und Eigenvektor eines Matrixvektorprodukts
bestimmt werden. Im Sinne dieses Theorems setzen wir dazu
\begin{equation}
A := \Sigma_{yy}^{-1}\Sigma_{yx},\quad b := a,\quad B := \Sigma_{xy}
\end{equation}
und erhalten für den betreffenden Eigenwert
\begin{equation}
\lambda_a = b^TBAa = a^T\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx}a.
\end{equation}
als Lösung (Maximum) des restringierten Optimierungsproblems
\begin{equation}
\max_{b} \left(a^T\Sigma_{xy}b\right)^2 \mbox{ u.d.N. } b^T\Sigma_{yy}b = 1
\end{equation}

# Modellformulierung
\footnotesize
\underline{Beweis (fortgeführt)}

Schritt (2)

Basierend auf Schritt (1) verbleibt die Lösung des restringierten Optimierungsproblem
\begin{equation}\label{eq:kka_opt_3}
\phi_r^2 = \max_{a} a^T\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx}a
\mbox{ u.d.N. }
a^T\Sigma_{xx}a   = 1,\,
a_i^T\Sigma_{xx}a = 0,  i = 1,...,r-1
\end{equation}
Dazu halten wir zunächst fest, dass \eqref{eq:kka_opt_3} mit den Definitionen
von $\alpha_i$ und $K$ in der Definition der Kanonischen  Koeffizientenvektoren,
Variaten, und Korrelationen geschrieben werden kann als
\begin{equation}\label{eq:kka_opt_4}
\phi_r^2 = \max_{\alpha} \alpha^T KK^T \alpha
\mbox{ u.d.N. }
\alpha^T \alpha   = 1,\,
\alpha_i^T\alpha  = 0,  i = 1,...,r-1,
\end{equation}
denn
\tiny
\begin{align}
\begin{split}
\phi_r^2 & = \max_{a} a^T\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx}a
\mbox{ u.d.N. }
a^T\Sigma_{xx}a   = 1,
a_i^T\Sigma_{xx}a = 0 \Leftrightarrow
\\
\phi_r^2 & = \max_{a} a^T\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx}a
\mbox{ u.d.N. }
\alpha^T\Sigma_{xx}^{-1/2}\Sigma_{xx}\Sigma_{xx}^{-1/2}\alpha = 1,
\alpha^T_i\Sigma_{xx}^{-1/2}\Sigma_{xx}\Sigma_{xx}^{-1/2}\alpha = 0
\\
\phi_r^2 & = \max_{a} \alpha^T\Sigma_{xx}^{-1/2}\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx}\Sigma_{xx}^{-1/2}\alpha
\mbox{ u.d.N. }
\alpha^T\alpha = 1,
\alpha^T_i\alpha = 0
\\
\phi_r^2 & = \max_{a} \alpha^T\Sigma_{xx}^{-1/2}\Sigma_{xy}\Sigma_{yy}^{-1/2}\Sigma_{yy}^{-1/2}\Sigma_{yx}\Sigma_{xx}^{-1/2}\alpha
\mbox{ u.d.N. }
\alpha^T\alpha = 1,
\alpha^T_i\alpha = 0
\\
\phi_r^2 & = \max_{a} \alpha^TKK^T\alpha
\mbox{ u.d.N. }
\alpha^T\alpha = 1,
\alpha^T_i\alpha = 0
\end{split}
\end{align}

# Modellformulierung
\footnotesize
\underline{Beweis (fortgeführt)}

Dabei sind nach der betreffenden Definition die $\alpha_i$ die Eigenvektoren
von $KK^T$ mit den $i = 1,...,r-1$ größten Eigenwerten. Nach dem Theorem zur
Maximierung quadratischer Formen mit Nebenbedingungen ist die Lösung von
\eqref{eq:kka_opt_4} der größte Eigenwert von $KK^T$ mit seinem assoziierten
Eigenvektor. Die Nebenbedingung $\alpha_i^T\alpha = 0$ schränkt diese Wahl auf
den $r$t-größten Eigenwert und seinen assoziierten Eigenvektor $\alpha_r$ ein.
Mit der Definition von Eigenwerten und Eigenvektoren gilt also
\begin{equation}
\phi_r^2 = \alpha_r^T KK^T \alpha_r = \alpha_r^T \lambda_r \alpha_r = \lambda_r \alpha_r^T\alpha_r = \lambda_r.
\end{equation}
Wir haben also gezeigt, dass das restringierte Optimierungsproblem des Theorems
den Maximumwert $\phi_r = \lambda_r^{1/2}$ hat. Es bleibt zu zeigen, dass dieser
Maximumwert für $a_r$ und $b_r$ angenommen wird.

Schritt (3)

Einsetzen von $a_r$ und $b_r$ in $a^T\Sigma_{xy}b$ ergibt mit
\begin{equation}
K = A\Lambda B^T
\Leftrightarrow KB = A\Lambda B^TB
\Leftrightarrow KB = A\Lambda
\Leftrightarrow K\beta_r = \alpha_r\lambda_r^{1/2}
\end{equation}
dass
\begin{equation}
a^T_r\Sigma_{xy}b_r
= \alpha_r^T\Sigma_{xx}^{-1/2}\Sigma_{xy}\Sigma_{yy}^{-1/2}\beta_r
= \alpha_r^TK\beta_r
= \alpha_r^T\alpha_r\lambda_r^{1/2}
= \rho_r
\end{equation}
Also nimmt $a^T\Sigma_{xy}b$ bei $a_r$ und $b_r$ seinen restringierten Maximalwert $\lambda_r$ an.

$\hfill\Box$

# Modellformulierung
\textcolor{darkblue}{Simulationsbeispiel}

\footnotesize
Wir betrachten das Beispiel (vgl. @uurtio_2018a)
\begin{equation}
p(x) = N(x;0_4,I_4) \mbox{ und } p(y|x) = N(y; LX, G)
\end{equation}
mit
\begin{equation}
L := \begin{pmatrix*}[r] 0.0 & 0.0 & 1.0 & 0.0 \\ 1.0 & 0.0 & 0.0 & 0.0 \\ 0.0 & 0.0 & 0.0 & -1.0 \end{pmatrix*}
\mbox{ und }
G := \begin{pmatrix*}[r] 0.2 & 0.0 & 0.0 \\ 0.0 & 0.4 & 0.0 \\ 0.0 & 0.0 & 0.3 \end{pmatrix*}
\end{equation}
Hier gilt offenbar $m_x = 4, m_y = 3, m = 7$ und
\begin{align}
\begin{split}
y_1 & = \,\,\,\, x_3 + \varepsilon_1 \\
y_2 & = \,\,\,\, x_1 + \varepsilon_2 \\
y_3 & =        - x_4 + \varepsilon_3 \\
\end{split}
\end{align}

mit
\begin{equation}
x_1 \sim N(0,1), x_3 \sim N(0,1), x_4 \sim N(0,1)
\end{equation}
und
\begin{equation}
\varepsilon_1 \sim N(0,0.2), \varepsilon_2 \sim N(0,0.4), \varepsilon_3 \sim N(0,0.3)
\end{equation}

# Modellformulierung
\textcolor{darkblue}{Simulationsbeispiel}

\footnotesize
Mit dem Theorem zu gemeinsamen Normalverteilungen (vgl. Einheit (3) Matrizen)
ergibt sich, dass
\begin{equation}
\begin{pmatrix} x \\ y \end{pmatrix}
\sim N(0_7,\Sigma)
\end{equation}
mit
\begin{equation}
\Sigma
=
\begin{pmatrix}
\Sigma_{xx} & \Sigma_{xy} \\
\Sigma_{yx} & \Sigma_{yy}
\end{pmatrix},
\end{equation}
wobei
\begin{equation}
\Sigma_{xx} = I_4, \quad
\Sigma_{xy} = L^T, \quad
\Sigma_{yx} = L \mbox{ und }
\Sigma_{yy} = G + LL^T.
\end{equation}
Explizit ergibt sich also
\begin{equation}
\Sigma
 =
\begin{pmatrix}
I_4 &  L^T \\
L   &  G + LL^T
\end{pmatrix}
=
\begin{pmatrix*}[r]
1.0 & 0.0 & 0.0 &  0.0 & 0.0 & 1.0 &  0.0 \\
0.0 & 1.0 & 0.0 &  0.0 & 0.0 & 0.0 &  0.0 \\
0.0 & 0.0 & 1.0 &  0.0 & 1.0 & 0.0 &  0.0 \\
0.0 & 0.0 & 0.0 &  1.0 & 0.0 & 0.0 & -1.0 \\
0.0 & 0.0 & 1.0 &  0.0 & 1.2 & 0.0 &  0.0 \\
1.0 & 0.0 & 0.0 &  0.0 & 0.0 & 1.4 &  0.0 \\
0.0 & 0.0 & 0.0 & -1.0 & 0.0 & 0.0 &  1.3 \\
\end{pmatrix*}
\end{equation}

# Modellformulierung
\textcolor{darkblue}{Simulationsbeispiel}
\vspace{2mm}

\tiny
\setstretch{1.1}
```{r, warning = F, message=F}
# R Pakete für Matrizenrechnung
library(matlib)
library(expm)

# Modellparameter
L = matrix(c(0,0,1, 0,
             1,0,0, 0,
             0,0,0,-1),
           nrow  = 3,
           byrow = T)
G = diag(c(0.2,0.4,0.3))

# Kovarianzmatrixpartition
Sigma_xx = diag(4)
Sigma_xy = t(L)
Sigma_yx = L
Sigma_yy = G + L %*% t(L)
Sigma    = rbind(cbind(Sigma_xx, Sigma_xy), cbind(Sigma_yx, Sigma_yy))
print(Sigma)
```
# Modellformulierung
\textcolor{darkblue}{Simulationsbeispiel}
\vspace{2mm}
\tiny
\setstretch{1.2}

```{r}
# Evaluation der iten kanonischen Koeffizientenvektoren und Korrelationen
K      = sqrtm(inv(Sigma_xx)) %*% Sigma_xy %*% sqrtm(inv(Sigma_yy)) # K
ALB    = svd(K)                                                     # K = A\LambdaV
A      = ALB$u                                                      # A
Lambda = ALB$d                                                      # Lambda
B      = ALB$v                                                      # B
rho    = Lambda                                                     # \rho_i = \lambda_i^{1/2}
a      = sqrtm(inv(Sigma_xx)) %*% A                                 # a_i = \Sigma_{xx}^{-1/2}\alpha_i
b      = sqrtm(inv(Sigma_yy)) %*% B                                 # b_i = \Sigma_{yy}^{-1/2}\beta_i
```

\footnotesize
Die kanonische Korrelationen und kanonischen Koeffizientenvektoren  ergeben sich  zu

```{r, echo = F}
# Ausgabe
cat("rho_1 = ", rho[1],", a_1^T = (", a[,1], "), b_1^T = (", b[,1], ")",
    "\nrho_2 = ", rho[2],", a_2^T = (", a[,2],") , b_2^T = (", b[,2], ")",
    "\nrho_3 = ", rho[3],", a_3^T = (", a[,3], "), b_3^T = (", b[,3], ")")

```


#
\setstretch{2.5}
\vfill
\large
Korrelation

Algebraische Grundlagen

Wahrscheinlichkeitstheoretische Grundlagen

Modellformulierung

**Modellschätzung**

Selbstkontrollfragen
\vfill

# Modellschätzung

\footnotesize
\begin{definition}[Schätzer kanonischer Korrelationen und Koeffizientenvektoren]
Für $i = 1,...,n$ seien
\begin{equation}
z_i = \begin{pmatrix} x_i \\ y_i \end{pmatrix}
\mbox{ mit }
\mathbb{E}(z_i) := 0_m
\mbox{ und }
\mathbb{C}(z_i) :=
\begin{pmatrix}
\Sigma_{xx} & \Sigma_{xy} \\
\Sigma_{yx} & \Sigma_{yy} \\
\end{pmatrix}
\in \mathbb{R}^{m \times m}
\end{equation}
unabhängig und identisch verteilte  $m$-dimensionale partitionierte
Zufallsvektoren sowie ihr Erwartungswert und ihre Kovarianzmatrix, respektive, und
\begin{equation}
C :=
\begin{pmatrix}
C_{xx} & C_{xy} \\
C_{yx} & C_{yy} \\
\end{pmatrix}
\in \mathbb{R}^{m \times m}
\end{equation}
sei ihre Stichprobenkovarianzmatrix. Dann sind für $i = 1,...,k := \min \{m_x,m_y\}$
\begin{equation}
\hat{a}_i := C_{xx}^{-1/2}\hat{\alpha}_i \in \mathbb{R}^{m_x}, \quad
\hat{b}_i := C_{yy}^{-1/2}\hat{\beta}_i \in \mathbb{R}^{m_y} \mbox{ und }
\hat{\rho}_i := \hat{\lambda}_i^{1/2}
\end{equation}
Schätzer der $i$ten kanonischen Koeffizientenvektoren und kanonischen Korrelationen,
respektive. Dabei sind mit
\begin{equation}
\hat{K} := C_{xx}^{-1/2}C_{xy}C_{yy}^{-1/2} \in \mathbb{R}^{m_x \times m_y}
\end{equation}
$\hat{\alpha}_i$ und $\hat{\lambda}_i$ der $i$te Eigenvektor und sein zugehöriger
Eigenwert von $\hat{K}\hat{K}^T$ und $\hat{\beta}_i$ der entsprechende Eigenkvektor
von $\hat{K}^T\hat{K}$.
\end{definition}

Bemerkungen

* Zur Modellschätzung wird  $\mathbb{C}(z)$ also durch $C$ ersetzt.

# Modellschätzung
\textcolor{darkblue}{Simulationsbeispiel}
\vspace{2mm}

\tiny
\setstretch{1.2}
```{r}
# R Pakete
library(MASS)
library(matlib)
library(expm)

# Modellparameter
m_x      = 4
m_y      = 3
k        = min(m_x,m_y)
L        = matrix(c(0,0,1,0,1,0,0,0,0,0,0,-1), nrow = 3,byrow = 3)
G        = diag(c(0.2,0.4,0.3))
Sigma_xx = diag(4)
Sigma_xy = t(L)
Sigma_yx = L
Sigma_yy = G + L %*% t(L)
Sigma    = rbind(cbind(Sigma_xx, Sigma_xy), cbind(Sigma_yx, Sigma_yy))
K        = sqrtm(inv(Sigma_xx)) %*% Sigma_xy %*% sqrtm(inv(Sigma_yy))
ALB      = svd(K)
A        = ALB$u
Lambda   = ALB$d
B        = ALB$v
rho      = Lambda
a        = sqrtm(inv(Sigma_xx)) %*% A
b        = sqrtm(inv(Sigma_yy)) %*% B
```

# Modellschätzung
\textcolor{darkblue}{Simulationsbeispiel}

\vspace{2mm}
\tiny
\setstretch{1.2}

```{r}
# Simulationen
n       = 1e1:1e3
rho_hat = matrix(rep(NaN, length(n)*k)  , nrow = k)
a_1_hat = matrix(rep(NaN, length(n)*m_x), nrow = m_x)
for(i in 1:length(n)){

    # Datengeneration
    Y          = t(mvrnorm(n[i],rep(0, m_x+m_y),Sigma))
    I_n        = diag(n[i])
    J_n        = matrix(rep(1,n[i]^2), nrow = n[i])

    # Stichprobenkovarianzmatrixpartition
    C          = (1/(n[i]-1))*(Y %*% (I_n-(1/n[i])*J_n) %*% t(Y))
    C_xx       = C[1:m_x,1:m_x]
    C_xy       = C[1:m_x,(m_x+1):(m_x+m_y)]
    C_yx       = C[(m_x+1):(m_x+m_y),1:m_x]
    C_yy       = C[(m_x+1):(m_x+m_y),(m_x+1):(m_x+m_y)]

    # Kanonische Korrelationsanalyse
    K_hat        = sqrtm(inv(C_xx)) %*% C_xy %*% sqrtm(inv(C_yy))
    ALB_hat      = svd(K_hat)
    A_hat        = ALB_hat$u
    Lambda_hat   = ALB_hat$d
    B_hat        = ALB_hat$v
    a_hat        = sqrtm(inv(C_xx)) %*% A_hat
    b_hat        = sqrtm(inv(C_yy)) %*% B_hat
    rho_hat[,i]  = as.matrix(Lambda_hat)
    a_1_hat[,i]  = a_hat[,1]
}
```


# Modellschätzung

\textcolor{darkblue}{Simulationsbeispiel}
\vspace{3mm}

```{r, echo = F, eval = F}
graphics.off()
library(latex2exp)
fdir        =  file.path(getwd(), "7_Abbildungen")
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "m",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.2)
cols        = c("gray20", "gray50", "gray80")
matplot(
n,
t(rho_hat),
type = "l",
col  = cols,
lty  = 1,
ylim = c(0.7,1),
ylab = "",
main = "Schätzung kanonischer Korrelationen")
for(i in 1:k){
  abline(h = rho[i], col = cols[i])
}
legend(
"bottomright",
c(TeX("$\\rho_1,\\hat{\\rho}_1$"),
  TeX("$\\rho_2,\\hat{\\rho}_2$"),
  TeX("$\\rho_3,\\hat{\\rho}_3$")),
lty = 1,
col = cols,
bty = "n")
dev.copy2pdf(
file        = file.path(fdir, "mvda_7_modellschätzung_1.pdf"),
width       = 7,
height      = 5)
```

```{r, echo = FALSE, out.width = "85%"}
knitr::include_graphics("7_Abbildungen/mvda_7_modellschätzung_1.pdf")
```


# Modellschätzung

\textcolor{darkblue}{Simulationsbeispiel}
\vspace{3mm}

```{r, echo = F, eval = F}
graphics.off()
fdir        =  file.path(getwd(), "7_Abbildungen")
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "m",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.2)
cols        = c("gray20", "gray50", "darkblue","gray80")
matplot(
n,
t(abs(a_1_hat)),
type = "l",
col  = cols,
lty  = 1,
ylim = c(-.1,1.2),
ylab = "",
main = "Schätzung der Absolutwerte des 1. kanonischen Koeffizientenvektors")
for(i in 1:m_x){
  abline(h = abs(a[i,1]), col = cols[i])
}
legend(
"right",
c(TeX("$|a_{11}|, |\\hat{a}_{11}|$"),
  TeX("$|a_{12}|, |\\hat{a}_{12}|$"),
  TeX("$|a_{13}|, |\\hat{a}_{13}|$"),
  TeX("$|a_{14}|, |\\hat{a}_{14}|$")),
lty = 1,
col = cols,
bty = "n")
dev.copy2pdf(
file        = file.path(fdir, "mvda_7_modellschätzung_2.pdf"),
width       = 7,
height      = 5)
```

```{r, echo = FALSE, out.width = "85%"}
knitr::include_graphics("7_Abbildungen/mvda_7_modellschätzung_2.pdf")
```


# Modelschätzung
\textcolor{darkblue}{Anwendungsbeispiel}
\vspace{3mm}
```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("7_Abbildungen/mvda_7_beispielszenario_kanonische_korrelation.pdf")
```

# Modelschätzung
\textcolor{darkblue}{Anwendungsbeispiel}

\footnotesize
$i = 1,...,n$ Patient:innen

\center
$y_{1i}$ BDI Score Reduktion, $y_{2i}$ Glucocorticoid Reduktion, $x_{1i}$ Therapiedauer, $x_{2i}$ Erfahrung Psychotherapeut:in, 

\setstretch{1}
```{r, echo = F}
fname       = file.path(getwd(), "7_Kanonische_Korrelationsanalyse.csv")
D           = read.table(fname, sep = ",", header = TRUE)
knitr::kable(D, "pipe")
```


# Modellschätzung
\vspace{2mm}
\textcolor{darkblue}{Anwendungsbeispiel}
\vspace{1mm}
\tiny
\setstretch{.9}
```{r, message = F}
# libraries
library(expm)
library(matlib)

# Datenpräprozessierung
fname      = file.path(getwd(), "7_Kanonische_Korrelationsanalyse.csv")
D          = read.table(fname, sep = ",", header = TRUE)
x          = as.matrix(cbind(D$x_1i, D$x_2i))
y          = as.matrix(cbind(D$y_1i, D$y_2i))
n          = nrow(x)
m_x        = ncol(x)
m_y        = ncol(y)
Y          = t(cbind(x,y))

# Stichprobenkovarianzmatrixpartition
I_n        = diag(n)
J_n        = matrix(rep(1,n^2), nrow = n)
C          = (1/(n-1))*(Y %*% (I_n-(1/n)*J_n) %*% t(Y))
C_xx       = C[1:m_x,1:m_x]
C_xy       = C[1:m_x,(m_x+1):(m_x+m_y)]
C_yx       = C[(m_x+1):(m_x+m_y),1:m_x]
C_yy       = C[(m_x+1):(m_x+m_y),(m_x+1):(m_x+m_y)]

# Kanonische Korrelationsanalyse
K_hat      = sqrtm(inv(C_xx)) %*% C_xy %*% sqrtm(inv(C_yy))
ALB_hat    = svd(K_hat)
A_hat      = ALB_hat$u
Lambda_hat = ALB_hat$d
B_hat      = ALB_hat$v
a_hat      = sqrtm(inv(C_xx)) %*% A_hat
b_hat      = sqrtm(inv(C_yy)) %*% B_hat
rho_hat    = as.matrix(Lambda_hat)

print(ALB_hat)
```

```{r, echo = F}
# Ausgabe
cat("rho_hat_1 : "  , rho_hat[1],
    "\na_hat_1   : ", a_hat[,1],
    "\nb_hat_1   : ", b_hat[,1],
    "\nrho_hat_2 : ", rho_hat[2],
    "\na_hat_2   : ", a_hat[,2],
    "\nb_hat_2   : ", b_hat[,2])
```

# Modellschätzung
\textcolor{darkblue}{Anwendungsbeispiel}
\small

Kanonische Korrelationsanalyse mir R's `cancor()` Funktion
\vspace{1mm}

\footnotesize
\setstretch{1}

```{r, message = F}
# Datenpräprozessierung
fname      = file.path(getwd(), "7_Kanonische_Korrelationsanalyse.csv")
D          = read.table(fname, sep = ",", header = TRUE)
x          = as.matrix(cbind(D$x_1i, D$x_2i))
y          = as.matrix(cbind(D$y_1i, D$y_2i))
cca        = cancor(x,y)
```

```{r, echo = F}
cat(  "rho_hat_1 : ", cca$cor[1],
    "\nrho_hat_2 : ", cca$cor[2])
```

# Modellschätzung
\textcolor{darkblue}{Anwendungsbeispiel}

\vspace{2mm}

\small
Die geschätzte maximale Korrelation von Linearkombinationen von $(x_1,x_2)$ und $(y_1,y_2)$ ist 0.99.

* $(x_1,x_2)$ und $(y_1,y_2)$ sind multivariat also "hochgradig" korreliert.

Basierend auf der simulationsvalidierten Schätzung ergibt sich

* $\xi       = 0.16x_1 + 0.17x_2$ als "bester Prädiktor"
* $\upsilon  = 0.15y_1 + 0.05y_2$ als "am besten prädizierbares Kriterium"

"Therapiedauer" und "Therapeut:innenerfahrung" scheinen zur bestmöglichen 
Prädiktion der Therapiegüte also in etwa gleichbedeutend, bei dem bestprädizierbarem
Kriterium der Therapiegüte trägt "BDI Score Reduktion" etwas mehr bei als 
"Glucocorticoid Reduktion" bei.


#
\setstretch{2.5}
\vfill
\large
Korrelation

Algebraische Grundlagen

Wahrscheinlichkeitstheoretische Grundlagen

Modellformulierung

Modellschätzung

**Selbstkontrollfragen**

# Selbskontrollfragen
\footnotesize
\setstretch{2.5}

1. Definieren Sie den Begriff der Korrelation.
2. Definieren Sie den Begriff der Stichprobenkorrelation.
3. Geben Sie das Theorem zu Korrelation und linear-affiner Abhängigkeit wieder.
4. Geben Sie das Theorem zu Kovarianz und Korrelation bei linear-affinen Transformationen wieder.
5. Erläutern Sie das Anwendungsszenario einer Kanonischen Korrelationsanalyse.
6. Erläutern Sie das Ziel einer Kanonischen Korrelationsanalyse.
7. Erläutern Sie die Begriffe "bester Prädiktor" und "am besten prädizierbares Kriterium". 
8. Geben Sie die Definition Kanonischer Koeffizientenvektoren, Variate und Korrelationen an.
9. Geben Sie das Theorem zu den Eigenschaften kanonischer Korrelationen und Variate wieder.
10. Geben Sie die Definition für Schätzer kanonischer Korrelationen und Koeffizientenvektoren wieder.
11. Skizzieren Sie die Durchführung einer kanonischen Korrelationsanalyse.

# Referenzen
\footnotesize